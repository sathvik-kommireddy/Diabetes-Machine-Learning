SVM RESULTS

 diabetes.data<- read.csv(file="C:/Users/sathv/OneDrive/Stats Research Project/diabetes_prediction_dataset.csv", 
+ header=TRUE, sep=",")
> 
> diabetes.data$smoking_history<- ifelse(diabetes.data$smoking_history %in% c("current", "former"),1,0)  
> diabetes.data$gender<- ifelse(diabetes.data$gender=='Male',1,0)
>                                        
> 
> #SPLITTING DATA INTO 80% TRAINING AND 20% TESTING SETS 
> set.seed(966452)
> sample <- sample(c(TRUE, FALSE), nrow(diabetes.data), replace=TRUE, prob=c(0.8,0.2))
> train<- diabetes.data[sample,]
> test<- diabetes.data[!sample,]
> 
> train.x<- data.matrix(train[-9])
> train.y<- data.matrix(train[9])
> test.x<- data.matrix(test[-9])
> test.y<- data.matrix(test[9])
> 
> install.packages("e1071")
WARNING: Rtools is required to build R packages but is not currently installed. Please download and install the appropriate version of Rtools before proceeding:

https://cran.rstudio.com/bin/windows/Rtools/
Installing package into ‘C:/Users/sathv/AppData/Local/R/win-library/4.5’
(as ‘lib’ is unspecified)
trying URL 'https://cran.rstudio.com/bin/windows/contrib/4.5/e1071_1.7-16.zip'
Content type 'application/zip' length 679083 bytes (663 KB)
downloaded 663 KB

package ‘e1071’ successfully unpacked and MD5 sums checked

The downloaded binary packages are in
	C:\Users\sathv\AppData\Local\Temp\RtmpugYqu6\downloaded_packages
> library(e1071)
> 
> #FITTING SVM WITH LINEAR KERNEL
> svm.class<- svm(as.factor(diabetes) ~ gender + age + hypertension + heart_disease + smoking_history + bmi + HbA1c_level + blood_glucose_level, 
+ data=train, kernel="linear")
> 
> #computing prediction accuracy for testing data
> pred.y<-as.numeric(predict(svm.class, test.x))-1
> match <- numeric(length(pred.y))
> 
> for (i in 1:length(pred.y))
+   match[i]<- ifelse(test.y[i]==pred.y[i], 1,0)
> print(paste("accuracy=", round(mean(match), digits=4)))
[1] "accuracy= 0.9609"
> 
> #calculating confusion matrix
> tp <- sum(pred.y == 1 & test.y == 1)
> fp <- sum(pred.y == 1 & test.y == 0)
> tn <- sum(pred.y == 0 & test.y == 0)
> fn <- sum(pred.y == 0 & test.y == 1)
> total <- length(test.y)
> 
> # Metrics
> accuracy <- (tp + tn) / total
> misclassrate <- (fp + fn) / total
> sensitivity <- tp / (tp + fn)
> FNR <- fn / (tp + fn)
> specificity <- tn / (fp + tn)
> FPR <- fp / (fp + tn)
> precision <- tp / (tp + fp)
> NPV <- tn / (fn + tn)
> F1score <- 2 * tp / (2 * tp + fn + fp)
> 
> # Print results
> print("Linear Kernel Confusion Matrix Results:")
[1] "Linear Kernel Confusion Matrix Results:"
> print(paste("TP =", tp, " FP =", fp, " TN =", tn, " FN =", fn, "Total =", total))
[1] "TP = 1013  FP = 87  TN = 18542  FN = 709 Total = 20351"
> print(paste("Accuracy =", round(accuracy, 4)))
[1] "Accuracy = 0.9609"
> print(paste("MisclassRate =", round(misclassrate, 4)))
[1] "MisclassRate = 0.0391"
> print(paste("Sensitivity =", round(sensitivity, 4)))
[1] "Sensitivity = 0.5883"
> print(paste("FNR =", round(FNR, 4)))
[1] "FNR = 0.4117"
> print(paste("Specificity =", round(specificity, 4)))
[1] "Specificity = 0.9953"
> print(paste("FPR =", round(FPR, 4)))
[1] "FPR = 0.0047"
> print(paste("Precision =", round(precision, 4)))
[1] "Precision = 0.9209"
> print(paste("NPV =", round(NPV, 4)))
[1] "NPV = 0.9632"
> print(paste("F1 Score =", round(F1score, 4)))
[1] "F1 Score = 0.7179"
> 
> ####ROC CURVE CODE
> 
> #COMPUTING CONFUSION MATRICES AND PERFORMANCE MEASURES FOR TESTING SET
> #FOR A RANGE OF CUT-OFFS
> svm.class <- svm(as.factor(diabetes) ~ gender + age + hypertension + heart_disease + smoking_history + bmi + HbA1c_level + blood_glucose_level, data=train, kernel="linear", probability=TRUE)
> 
> svm.prob <- predict(svm.class, newdata=test, probability=TRUE)
> prob_attr <- attr(svm.prob, "probabilities")
> test$prob_1 <- prob_attr[, "1"]
> 
> tpos<- matrix(NA, nrow=nrow(test), ncol=102)
> fpos<- matrix(NA, nrow=nrow(test), ncol=102)
> tneg<- matrix(NA, nrow=nrow(test), ncol=102)
> fneg<- matrix(NA, nrow=nrow(test), ncol=102)
> 
> 
> for (i in 0:101) {
+   tpos[,i+1]<- ifelse(test$diabetes==1 & test$prob_1>=0.01*i,1,0)
+   fpos[,i+1]<- ifelse(test$diabetes==0 & test$prob_1>=0.01*i, 1,0)
+   tneg[,i+1]<- ifelse(test$diabetes==0 & test$prob_1<0.01*i,1,0)
+   fneg[,i+1]<- ifelse(test$diabetes==1 & test$prob_1<0.01*i,1,0)
+ }
> 
> tp<- c()
> fp<- c()
> tn<- c()
> fn<- c()
> accuracy<- c()
> misclassrate<- c()
> sensitivity<- c()
> specificity<- c()
> oneminusspec<- c()
> cutoff<- c()
> 
> 
> for (i in 1:102) {
+   tp[i]<- sum(tpos[,i])
+   fp[i]<- sum(fpos[,i])
+   tn[i]<- sum(tneg[,i])
+   fn[i]<- sum(fneg[,i])
+   total<- nrow(test)
+   accuracy[i]<- (tp[i]+tn[i])/total
+   misclassrate[i]<- (fp[i]+fn[i])/total
+   sensitivity[i]<- tp[i]/(tp[i]+fn[i])
+   specificity[i]<- tn[i]/(fp[i]+tn[i])
+   oneminusspec[i]<- fp[i]/(fp[i]+tn[i])
+   cutoff[i]<- 0.01*(i-1)
+ }
> 
> #PLOTTING ROC CURVE
> plot(oneminusspec, sensitivity, type="l", lty=1, main="The Receiver 
+ Operating Characteristic Curve", xlab="1-Specificity", ylab="Sensitivity")
> points(oneminusspec, sensitivity, pch=0) #pch=plot character, 0=square
> 
> #REPORTING MEASURES FOR THE POINT ON ROC CURVE CLOSEST TO THE IDEAL POINT (0,1)
> distance<- c()
> for (i in 1:102)
+   distance[i]<- sqrt(oneminusspec[i]^2+(1-sensitivity[i])^2)
> 
> measures<- cbind(accuracy, misclassrate, sensitivity, specificity, distance, cutoff, tp, fp, tn, fn)
> min.dist<- min(distance)
> print(measures)
         accuracy misclassrate sensitivity specificity  distance cutoff   tp    fp    tn   fn
  [1,] 0.08461501   0.91538499   1.0000000   0.0000000 1.0000000   0.00 1722 18629     0    0
  [2,] 0.65131935   0.34868065   0.9907085   0.6199474 0.3801662   0.01 1706  7080 11549   16
  [3,] 0.73554125   0.26445875   0.9709640   0.7137796 0.2876894   0.02 1672  5332 13297   50
  [4,] 0.78428578   0.21571422   0.9547038   0.7685329 0.2358575   0.03 1644  4312 14317   78
  [5,] 0.81696231   0.18303769   0.9349593   0.8060551 0.2045603   0.04 1610  3613 15016  112
  [6,] 0.84123630   0.15876370   0.9169570   0.8342369 0.1854010   0.05 1579  3088 15541  143
  [7,] 0.85941723   0.14058277   0.8977933   0.8558699 0.1766910   0.06 1546  2685 15944  176
  [8,] 0.87209474   0.12790526   0.8774681   0.8715980 0.1774856   0.07 1511  2392 16237  211
  [9,] 0.88310157   0.11689843   0.8600465   0.8852327 0.1809932   0.08 1481  2138 16491  241
 [10,] 0.89160238   0.10839762   0.8472706   0.8957003 0.1849451   0.09 1459  1943 16686  263
 [11,] 0.89951354   0.10048646   0.8350755   0.9054700 0.1900948   0.10 1438  1761 16868  284
 [12,] 0.90624539   0.09375461   0.8269454   0.9135756 0.1934349   0.11 1424  1610 17019  298
 [13,] 0.91204363   0.08795637   0.8141696   0.9210908 0.2018901   0.12 1402  1470 17159  320
 [14,] 0.91744877   0.08255123   0.8077816   0.9275860 0.2054061   0.13 1391  1349 17280  331
 [15,] 0.92192030   0.07807970   0.7990708   0.9332761 0.2117182   0.14 1376  1243 17386  346
 [16,] 0.92545821   0.07454179   0.7897793   0.9379999 0.2191729   0.15 1360  1155 17474  362
 [17,] 0.92850474   0.07149526   0.7804878   0.9421869 0.2269977   0.16 1344  1077 17552  378
 [18,] 0.93164955   0.06835045   0.7723577   0.9463739 0.2338734   0.17 1330   999 17630  392
 [19,] 0.93395902   0.06604098   0.7636469   0.9497021 0.2416457   0.18 1315   937 17692  407
 [20,] 0.93695641   0.06304359   0.7590012   0.9534060 0.2454617   0.19 1307   868 17761  415
 [21,] 0.93911847   0.06088153   0.7531940   0.9563047 0.2506442   0.20 1297   814 17815  425
 [22,] 0.94078915   0.05921085   0.7450639   0.9588813 0.2582309   0.21 1283   766 17863  439
 [23,] 0.94196845   0.05803155   0.7392567   0.9607064 0.2636874   0.22 1273   732 17897  449
 [24,] 0.94368827   0.05631173   0.7357724   0.9629073 0.2668185   0.23 1267   691 17938  455
 [25,] 0.94555550   0.05444450   0.7293844   0.9655376 0.2728011   0.24 1256   642 17987  466
 [26,] 0.94702963   0.05297037   0.7259001   0.9674701 0.2760234   0.25 1250   606 18023  472
 [27,] 0.94840548   0.05159452   0.7224158   0.9692952 0.2792772   0.26 1244   572 18057  478
 [28,] 0.94958479   0.05041521   0.7148664   0.9712813 0.2865762   0.27 1231   535 18094  491
 [29,] 0.95066581   0.04933419   0.7102207   0.9728917 0.2910445   0.28 1223   505 18124  499
 [30,] 0.95164857   0.04835143   0.7049942   0.9744484 0.2961103   0.29 1214   476 18153  508
 [31,] 0.95243477   0.04756523   0.6991870   0.9758441 0.3017813   0.30 1204   450 18179  518
 [32,] 0.95341752   0.04658248   0.6933798   0.9774545 0.3074480   0.31 1194   420 18209  528
 [33,] 0.95449855   0.04550145   0.6922184   0.9787428 0.3085148   0.32 1192   396 18233  530
 [34,] 0.95538303   0.04461697   0.6893148   0.9799775 0.3113298   0.33 1187   373 18256  535
 [35,] 0.95675888   0.04324112   0.6869919   0.9816952 0.3135429   0.34 1183   341 18288  539
 [36,] 0.95705371   0.04294629   0.6823461   0.9824467 0.3181385   0.35 1175   327 18302  547
 [37,] 0.95734853   0.04265147   0.6777003   0.9831982 0.3227373   0.36 1167   313 18316  555
 [38,] 0.95769250   0.04230750   0.6747967   0.9838424 0.3256044   0.37 1162   301 18328  560
 [39,] 0.95808560   0.04191440   0.6718931   0.9845402 0.3284709   0.38 1157   288 18341  565
 [40,] 0.95788905   0.04211095   0.6660859   0.9848623 0.3342570   0.39 1147   282 18347  575
 [41,] 0.95852784   0.04147216   0.6637631   0.9857749 0.3365377   0.40 1143   265 18364  579
 [42,] 0.95887180   0.04112820   0.6585366   0.9866337 0.3417249   0.41 1134   249 18380  588
 [43,] 0.95906835   0.04093165   0.6521487   0.9874389 0.3480781   0.42 1123   234 18395  599
 [44,] 0.95955973   0.04044027   0.6492451   0.9882441 0.3509519   0.43 1118   219 18410  604
 [45,] 0.95990369   0.04009631   0.6434379   0.9891567 0.3567270   0.44 1108   202 18427  614
 [46,] 0.96000197   0.03999803   0.6393728   0.9896398 0.3607760   0.45 1101   193 18436  621
 [47,] 0.96029679   0.03970321   0.6382114   0.9900692 0.3619249   0.46 1099   185 18444  623
 [48,] 0.96054248   0.03945752   0.6347271   0.9906597 0.3653923   0.47 1093   174 18455  629
 [49,] 0.96108299   0.03891701   0.6312427   0.9915723 0.3688536   0.48 1087   157 18472  635
 [50,] 0.96098472   0.03901528   0.6260163   0.9919480 0.3740704   0.49 1078   150 18479  644
 [51,] 0.96103386   0.03896614   0.6213705   0.9924312 0.3787051   0.50 1070   141 18488  652
 [52,] 0.96098472   0.03901528   0.6161440   0.9928606 0.3839224   0.51 1061   133 18496  661
 [53,] 0.96093558   0.03906442   0.6126597   0.9931290 0.3874012   0.52 1055   128 18501  667
 [54,] 0.96093558   0.03906442   0.6080139   0.9935584 0.3920390   0.53 1047   120 18509  675
 [55,] 0.96103386   0.03896614   0.6062718   0.9938268 0.3937766   0.54 1044   115 18514  678
 [56,] 0.96108299   0.03891701   0.6033682   0.9941489 0.3966750   0.55 1039   109 18520  683
 [57,] 0.96108299   0.03891701   0.6016260   0.9943099 0.3984146   0.56 1036   106 18523  686
 [58,] 0.96103386   0.03896614   0.5987224   0.9945247 0.4013149   0.57 1031   102 18527  691
 [59,] 0.96103386   0.03896614   0.5958188   0.9947931 0.4042147   0.58 1026    97 18532  696
 [60,] 0.96093558   0.03906442   0.5911731   0.9951151 0.4088561   0.59 1018    91 18538  704
 [61,] 0.96073903   0.03926097   0.5842044   0.9955446 0.4158195   0.60 1006    83 18546  716
 [62,] 0.96093558   0.03906442   0.5807201   0.9960814 0.4192982   0.61 1000    73 18556  722
 [63,] 0.96054248   0.03945752   0.5731707   0.9963498 0.4268449   0.62  987    68 18561  735
 [64,] 0.96054248   0.03945752   0.5714286   0.9965108 0.4285856   0.63  984    65 18564  738
 [65,] 0.96024765   0.03975235   0.5644599   0.9968329 0.4355516   0.64  972    59 18570  750
 [66,] 0.95995283   0.04004717   0.5603949   0.9968866 0.4396161   0.65  965    58 18571  757
 [67,] 0.95995283   0.04004717   0.5569106   0.9972087 0.4430982   0.66  959    52 18577  763
 [68,] 0.95946145   0.04053855   0.5481998   0.9974771 0.4518073   0.67  944    47 18582  778
 [69,] 0.95897008   0.04102992   0.5400697   0.9976918 0.4599361   0.68  930    43 18586  792
 [70,] 0.95892094   0.04107906   0.5365854   0.9979602 0.4634191   0.69  924    38 18591  798
 [71,] 0.95838042   0.04161958   0.5290360   0.9980675 0.4709680   0.70  911    36 18593  811
 [72,] 0.95818387   0.04181613   0.5255517   0.9981749 0.4744518   0.71  905    34 18595  817
 [73,] 0.95818387   0.04181613   0.5220674   0.9984970 0.4779350   0.72  899    28 18601  823
 [74,] 0.95808560   0.04191440   0.5174216   0.9988190 0.4825798   0.73  891    22 18607  831
 [75,] 0.95788905   0.04211095   0.5133566   0.9989801 0.4866445   0.74  884    19 18610  838
 [76,] 0.95744681   0.04255319   0.5075494   0.9990338 0.4924516   0.75  874    18 18611  848
 [77,] 0.95705371   0.04294629   0.5017422   0.9991411 0.4982586   0.76  864    16 18613  858
 [78,] 0.95656233   0.04343767   0.4947735   0.9992485 0.5052270   0.77  852    14 18615  870
 [79,] 0.95631664   0.04368336   0.4889663   0.9995169 0.5110339   0.78  842     9 18620  880
 [80,] 0.95577613   0.04422387   0.4802555   0.9997316 0.5197446   0.79  827     5 18624  895
 [81,] 0.95543217   0.04456783   0.4761905   0.9997316 0.5238096   0.80  820     5 18624  902
 [82,] 0.95498993   0.04501007   0.4703833   0.9997853 0.5296168   0.81  810     4 18625  912
 [83,] 0.95449855   0.04550145   0.4645761   0.9997853 0.5354240   0.82  800     4 18625  922
 [84,] 0.95420372   0.04579628   0.4610918   0.9997853 0.5389083   0.83  794     4 18625  928
 [85,] 0.95381062   0.04618938   0.4564460   0.9997853 0.5435540   0.84  786     4 18625  936
 [86,] 0.95263132   0.04736868   0.4425087   0.9997853 0.5574913   0.85  762     4 18625  960
 [87,] 0.95209081   0.04790919   0.4361208   0.9997853 0.5638793   0.86  751     4 18625  971
 [88,] 0.95140288   0.04859712   0.4279907   0.9997853 0.5720093   0.87  737     4 18625  985
 [89,] 0.95071495   0.04928505   0.4198606   0.9997853 0.5801394   0.88  723     4 18625  999
 [90,] 0.95037099   0.04962901   0.4157956   0.9997853 0.5842045   0.89  716     4 18625 1006
 [91,] 0.94973220   0.05026780   0.4070848   0.9998926 0.5929152   0.90  701     2 18627 1021
 [92,] 0.94879858   0.05120142   0.3960511   0.9998926 0.6039489   0.91  682     2 18627 1040
 [93,] 0.94737359   0.05262641   0.3792102   0.9998926 0.6207898   0.92  653     2 18627 1069
 [94,] 0.94624343   0.05375657   0.3652729   0.9999463 0.6347271   0.93  629     1 18628 1093
 [95,] 0.94506412   0.05493588   0.3507549   1.0000000 0.6492451   0.94  604     0 18629 1118
 [96,] 0.94354086   0.05645914   0.3327526   1.0000000 0.6672474   0.95  573     0 18629 1149
 [97,] 0.94113311   0.05886689   0.3042973   1.0000000 0.6957027   0.96  524     0 18629 1198
 [98,] 0.93902020   0.06097980   0.2793264   1.0000000 0.7206736   0.97  481     0 18629 1241
 [99,] 0.93543315   0.06456685   0.2369338   1.0000000 0.7630662   0.98  408     0 18629 1314
[100,] 0.93042111   0.06957889   0.1777003   1.0000000 0.8222997   0.99  306     0 18629 1416
 [ reached 'max' / getOption("max.print") -- omitted 2 rows ]
> 
> #COMPUTING AREA UNDER THE ROC CURVE
> sensitivity<- sensitivity[order(sensitivity)]
> oneminusspec<- oneminusspec[order(oneminusspec)]
> 
> library(Hmisc) #Harrell Miscellaneous packages

Attaching package: ‘Hmisc’

The following object is masked from ‘package:e1071’:

    impute

The following objects are masked from ‘package:base’:

    format.pval, units
> lagx<- Lag(oneminusspec,shift=1)
> lagy<- Lag(sensitivity, shift=1)
> lagx[is.na(lagx)]<- 0
> lagy[is.na(lagy)]<- 0
> trapezoid<- (oneminusspec-lagx)*(sensitivity+lagy)/2
> print(AUC<- sum(trapezoid))
[1] 0.9561169
> 
> 
> #FITTING SVM WITH POLYNOMIAL KERNEL
> svm.class<- svm(as.factor(diabetes) ~  gender + age + hypertension + heart_disease + smoking_history + bmi + HbA1c_level + blood_glucose_level, 
+ data=train, kernel="polynomial")
> 
> #computing prediction accuracy for testing data
> pred.y<- as.numeric(predict(svm.class, test.x))-1
> 
> for (i in 1:length(pred.y))
+   match[i]<- ifelse(test.y[i]==pred.y[i], 1,0)
> print(paste("accuracy=", round(mean(match), digits=4)))
[1] "accuracy= 0.9638"
> 
> #calculating confusion matrix
> tp <- sum(pred.y == 1 & test.y == 1)
> fp <- sum(pred.y == 1 & test.y == 0)
> tn <- sum(pred.y == 0 & test.y == 0)
> fn <- sum(pred.y == 0 & test.y == 1)
> total <- length(test.y)
> 
> # Metrics
> accuracy <- (tp + tn) / total
> misclassrate <- (fp + fn) / total
> sensitivity <- tp / (tp + fn)
> FNR <- fn / (tp + fn)
> specificity <- tn / (fp + tn)
> FPR <- fp / (fp + tn)
> precision <- tp / (tp + fp)
> NPV <- tn / (fn + tn)
> F1score <- 2 * tp / (2 * tp + fn + fp)
> 
> # Print results
> print("Polynomial Kernel Confusion Matrix Results:")
[1] "Polynomial Kernel Confusion Matrix Results:"
> print(paste("TP =", tp, " FP =", fp, " TN =", tn, " FN =", fn, "Total =", total))
[1] "TP = 1003  FP = 17  TN = 18612  FN = 719 Total = 20351"
> print(paste("Accuracy =", round(accuracy, 4)))
[1] "Accuracy = 0.9638"
> print(paste("MisclassRate =", round(misclassrate, 4)))
[1] "MisclassRate = 0.0362"
> print(paste("Sensitivity =", round(sensitivity, 4)))
[1] "Sensitivity = 0.5825"
> print(paste("FNR =", round(FNR, 4)))
[1] "FNR = 0.4175"
> print(paste("Specificity =", round(specificity, 4)))
[1] "Specificity = 0.9991"
> print(paste("FPR =", round(FPR, 4)))
[1] "FPR = 9e-04"
> print(paste("Precision =", round(precision, 4)))
[1] "Precision = 0.9833"
> print(paste("NPV =", round(NPV, 4)))
[1] "NPV = 0.9628"
> print(paste("F1 Score =", round(F1score, 4)))
[1] "F1 Score = 0.7316"
> 
> ####ROC CURVE CODE
> 
> #COMPUTING CONFUSION MATRICES AND PERFORMANCE MEASURES FOR TESTING SET
> #FOR A RANGE OF CUT-OFFS
> svm.class <- svm(as.factor(diabetes) ~ gender + age + hypertension + heart_disease + smoking_history + bmi + HbA1c_level + blood_glucose_level, data=train, kernel="polynomial", probability=TRUE)
> 
> svm.prob <- predict(svm.class, newdata=test, probability=TRUE)
> prob_attr <- attr(svm.prob, "probabilities")
> test$prob_1 <- prob_attr[, "1"]
> 
> tpos<- matrix(NA, nrow=nrow(test), ncol=102)
> fpos<- matrix(NA, nrow=nrow(test), ncol=102)
> tneg<- matrix(NA, nrow=nrow(test), ncol=102)
> fneg<- matrix(NA, nrow=nrow(test), ncol=102)
> 
> 
> for (i in 0:101) {
+   tpos[,i+1]<- ifelse(test$diabetes==1 & test$prob_1>=0.01*i,1,0)
+   fpos[,i+1]<- ifelse(test$diabetes==0 & test$prob_1>=0.01*i, 1,0)
+   tneg[,i+1]<- ifelse(test$diabetes==0 & test$prob_1<0.01*i,1,0)
+   fneg[,i+1]<- ifelse(test$diabetes==1 & test$prob_1<0.01*i,1,0)
+ }
> 
> tp<- c()
> fp<- c()
> tn<- c()
> fn<- c()
> accuracy<- c()
> misclassrate<- c()
> sensitivity<- c()
> specificity<- c()
> oneminusspec<- c()
> cutoff<- c()
> 
> 
> for (i in 1:102) {
+   tp[i]<- sum(tpos[,i])
+   fp[i]<- sum(fpos[,i])
+   tn[i]<- sum(tneg[,i])
+   fn[i]<- sum(fneg[,i])
+   total<- nrow(test)
+   accuracy[i]<- (tp[i]+tn[i])/total
+   misclassrate[i]<- (fp[i]+fn[i])/total
+   sensitivity[i]<- tp[i]/(tp[i]+fn[i])
+   specificity[i]<- tn[i]/(fp[i]+tn[i])
+   oneminusspec[i]<- fp[i]/(fp[i]+tn[i])
+   cutoff[i]<- 0.01*(i-1)
+ }
> 
> #PLOTTING ROC CURVE
> plot(oneminusspec, sensitivity, type="l", lty=1, main="The Receiver 
+ Operating Characteristic Curve", xlab="1-Specificity", ylab="Sensitivity")
> points(oneminusspec, sensitivity, pch=0) #pch=plot character, 0=square
> 
> #REPORTING MEASURES FOR THE POINT ON ROC CURVE CLOSEST TO THE IDEAL POINT (0,1)
> distance<- c()
> for (i in 1:102)
+   distance[i]<- sqrt(oneminusspec[i]^2+(1-sensitivity[i])^2)
> 
> measures<- cbind(accuracy, misclassrate, sensitivity, specificity, distance, cutoff, tp, fp, tn, fn)
> min.dist<- min(distance)
> print(measures)
         accuracy misclassrate sensitivity specificity  distance cutoff   tp    fp    tn   fn
  [1,] 0.08461501   0.91538499   1.0000000   0.0000000 1.0000000   0.00 1722 18629     0    0
  [2,] 0.44656282   0.55343718   0.9994193   0.3954587 0.6045416   0.01 1721 11262  7367    1
  [3,] 0.62488330   0.37511670   0.9773519   0.5923023 0.4083263   0.02 1683  7595 11034   39
  [4,] 0.72492752   0.27507248   0.9297329   0.7059960 0.3022843   0.03 1601  5477 13152  121
  [5,] 0.78787283   0.21212717   0.8931475   0.7781416 0.2462491   0.04 1538  4133 14496  184
  [6,] 0.83190015   0.16809985   0.8548200   0.8297815 0.2237221   0.05 1472  3171 15458  250
  [7,] 0.86433099   0.13566901   0.8344948   0.8670889 0.2122671   0.06 1437  2476 16153  285
  [8,] 0.88688517   0.11311483   0.8182346   0.8932310 0.2108039   0.07 1409  1989 16640  313
  [9,] 0.90953761   0.09046239   0.8025552   0.9194267 0.2132522   0.08 1382  1501 17128  340
 [10,] 0.92083927   0.07916073   0.7857143   0.9333298 0.2244177   0.09 1353  1242 17387  369
 [11,] 0.93366419   0.06633581   0.7706156   0.9487358 0.2350430   0.10 1327   955 17674  395
 [12,] 0.94560464   0.05439536   0.7584204   0.9629073 0.2444106   0.11 1306   691 17938  416
 [13,] 0.95115719   0.04884281   0.7526132   0.9695099 0.2492586   0.12 1296   568 18061  426
 [14,] 0.95425286   0.04574714   0.7462253   0.9734822 0.2551564   0.13 1285   494 18135  437
 [15,] 0.95715198   0.04284802   0.7439024   0.9768640 0.2571405   0.14 1281   431 18198  441
 [16,] 0.95882266   0.04117734   0.7375145   0.9792796 0.2633020   0.15 1270   386 18243  452
 [17,] 0.96024765   0.03975235   0.7293844   0.9815878 0.2712412   0.16 1256   343 18286  466
 [18,] 0.96123041   0.03876959   0.7235772   0.9831982 0.2769329   0.17 1246   313 18316  476
 [19,] 0.96216402   0.03783598   0.7195122   0.9845939 0.2809106   0.18 1239   287 18342  483
 [20,] 0.96349074   0.03650926   0.7171893   0.9862580 0.2831444   0.19 1235   256 18373  487
 [21,] 0.96398211   0.03601789   0.7137050   0.9871169 0.2865847   0.20 1229   240 18389  493
 [22,] 0.96447349   0.03552651   0.7113821   0.9878684 0.2888727   0.21 1225   226 18403  497
 [23,] 0.96535797   0.03464203   0.7096400   0.9889957 0.2905685   0.22 1222   205 18424  500
 [24,] 0.96580021   0.03419979   0.7073171   0.9896935 0.2928643   0.23 1218   192 18437  504
 [25,] 0.96604589   0.03395411   0.7038328   0.9902840 0.2963266   0.24 1212   181 18448  510
 [26,] 0.96624245   0.03375755   0.7015099   0.9907134 0.2986346   0.25 1208   173 18456  514
 [27,] 0.96629158   0.03370842   0.6968641   0.9911965 0.3032637   0.26 1200   164 18465  522
 [28,] 0.96634072   0.03365928   0.6939605   0.9915186 0.3061570   0.27 1195   158 18471  527
 [29,] 0.96619331   0.03380669   0.6916376   0.9915723 0.3084775   0.28 1191   157 18472  531
 [30,] 0.96668468   0.03331532   0.6893148   0.9923238 0.3107801   0.29 1187   143 18486  535
 [31,] 0.96653727   0.03346273   0.6852497   0.9925385 0.3148387   0.30 1180   139 18490  542
 [32,] 0.96673382   0.03326618   0.6840883   0.9928606 0.3159924   0.31 1178   133 18496  544
 [33,] 0.96702865   0.03297135   0.6829268   0.9932900 0.3171442   0.32 1176   125 18504  546
 [34,] 0.96722520   0.03277480   0.6811847   0.9936658 0.3188782   0.33 1173   118 18511  549
 [35,] 0.96727434   0.03272566   0.6794425   0.9938805 0.3206159   0.34 1170   114 18515  552
 [36,] 0.96747089   0.03252911   0.6788618   0.9941489 0.3211915   0.35 1169   109 18520  553
 [37,] 0.96752002   0.03247998   0.6765389   0.9944173 0.3235093   0.36 1165   104 18525  557
 [38,] 0.96761830   0.03238170   0.6759582   0.9945783 0.3240872   0.37 1164   101 18528  558
 [39,] 0.96747089   0.03252911   0.6736353   0.9946320 0.3264088   0.38 1160   100 18529  562
 [40,] 0.96756916   0.03243084   0.6730546   0.9947931 0.3269869   0.39 1159    97 18532  563
 [41,] 0.96761830   0.03238170   0.6724739   0.9949004 0.3275658   0.40 1158    95 18534  564
 [42,] 0.96791312   0.03208688   0.6724739   0.9952225 0.3275610   0.41 1158    89 18540  564
 [43,] 0.96791312   0.03208688   0.6701510   0.9954372 0.3298806   0.42 1154    85 18544  568
 [44,] 0.96771657   0.03228343   0.6666667   0.9955446 0.3333631   0.43 1148    83 18546  574
 [45,] 0.96771657   0.03228343   0.6643438   0.9957593 0.3356830   0.44 1144    79 18550  578
 [46,] 0.96796226   0.03203774   0.6643438   0.9960277 0.3356797   0.45 1144    74 18555  578
 [47,] 0.96796226   0.03203774   0.6637631   0.9960814 0.3362598   0.46 1143    73 18556  579
 [48,] 0.96801140   0.03198860   0.6620209   0.9962961 0.3379994   0.47 1140    69 18560  582
 [49,] 0.96776571   0.03223429   0.6585366   0.9963498 0.3414829   0.48 1134    68 18561  588
 [50,] 0.96776571   0.03223429   0.6567944   0.9965108 0.3432233   0.49 1131    65 18564  591
 [51,] 0.96781485   0.03218515   0.6567944   0.9965645 0.3432228   0.50 1131    64 18565  591
 [52,] 0.96791312   0.03208688   0.6562137   0.9967255 0.3438019   0.51 1130    61 18568  592
 [53,] 0.96781485   0.03218515   0.6550523   0.9967255 0.3449633   0.52 1128    61 18568  594
 [54,] 0.96771657   0.03228343   0.6533101   0.9967792 0.3467049   0.53 1125    60 18569  597
 [55,] 0.96761830   0.03238170   0.6509872   0.9968866 0.3490267   0.54 1121    58 18571  601
 [56,] 0.96722520   0.03277480   0.6463415   0.9968866 0.3536722   0.55 1113    58 18571  609
 [57,] 0.96732347   0.03267653   0.6445993   0.9971550 0.3554121   0.56 1110    53 18576  612
 [58,] 0.96727434   0.03272566   0.6434379   0.9972087 0.3565731   0.57 1108    52 18577  614
 [59,] 0.96722520   0.03277480   0.6422764   0.9972623 0.3577341   0.58 1106    51 18578  616
 [60,] 0.96712692   0.03287308   0.6399535   0.9973697 0.3600561   0.59 1102    49 18580  620
 [61,] 0.96712692   0.03287308   0.6376307   0.9975844 0.3623774   0.60 1098    45 18584  624
 [62,] 0.96712692   0.03287308   0.6353078   0.9977991 0.3646989   0.61 1094    41 18588  628
 [63,] 0.96707778   0.03292222   0.6335656   0.9979065 0.3664404   0.62 1091    39 18590  631
 [64,] 0.96678296   0.03321704   0.6289199   0.9980138 0.3710855   0.63 1083    37 18592  639
 [65,] 0.96683210   0.03316790   0.6283391   0.9981212 0.3716656   0.64 1082    35 18594  640
 [66,] 0.96678296   0.03321704   0.6260163   0.9982822 0.3739877   0.65 1078    32 18597  644
 [67,] 0.96668468   0.03331532   0.6236934   0.9983896 0.3763101   0.66 1074    30 18599  648
 [68,] 0.96648813   0.03351187   0.6213705   0.9983896 0.3786329   0.67 1070    30 18599  652
 [69,] 0.96634072   0.03365928   0.6196283   0.9983896 0.3803751   0.68 1067    30 18599  655
 [70,] 0.96619331   0.03380669   0.6178862   0.9983896 0.3821172   0.69 1064    30 18599  658
 [71,] 0.96609503   0.03390497   0.6167247   0.9983896 0.3832786   0.70 1062    30 18599  660
 [72,] 0.96594762   0.03405238   0.6149826   0.9983896 0.3850208   0.71 1059    30 18599  663
 [73,] 0.96584934   0.03415066   0.6132404   0.9984433 0.3867627   0.72 1056    29 18600  666
 [74,] 0.96560366   0.03439634   0.6091754   0.9985506 0.3908273   0.73 1049    27 18602  673
 [75,] 0.96550538   0.03449462   0.6074332   0.9986043 0.3925693   0.74 1046    26 18603  676
 [76,] 0.96555452   0.03444548   0.6074332   0.9986580 0.3925691   0.75 1046    25 18604  676
 [77,] 0.96545624   0.03454376   0.6051103   0.9987654 0.3948916   0.76 1042    23 18606  680
 [78,] 0.96521055   0.03478945   0.6022067   0.9987654 0.3977952   0.77 1037    23 18606  685
 [79,] 0.96491573   0.03508427   0.5981417   0.9988190 0.4018600   0.78 1030    22 18607  692
 [80,] 0.96491573   0.03508427   0.5975610   0.9988727 0.4024406   0.79 1029    21 18608  693
 [81,] 0.96476832   0.03523168   0.5940767   0.9990338 0.4059245   0.80 1023    18 18611  699
 [82,] 0.96462090   0.03537910   0.5923345   0.9990338 0.4076667   0.81 1020    18 18611  702
 [83,] 0.96447349   0.03552651   0.5905923   0.9990338 0.4094088   0.82 1017    18 18611  705
 [84,] 0.96432608   0.03567392   0.5888502   0.9990338 0.4111510   0.83 1014    18 18611  708
 [85,] 0.96427694   0.03572306   0.5876887   0.9990874 0.4123123   0.84 1012    17 18612  710
 [86,] 0.96393298   0.03606702   0.5836237   0.9990874 0.4163773   0.85 1005    17 18612  717
 [87,] 0.96363815   0.03636185   0.5795587   0.9991411 0.4204422   0.86  998    16 18613  724
 [88,] 0.96353988   0.03646012   0.5778165   0.9991948 0.4221843   0.87  995    15 18614  727
 [89,] 0.96334332   0.03665668   0.5749129   0.9992485 0.4250878   0.88  990    14 18615  732
 [90,] 0.96334332   0.03665668   0.5725900   0.9994632 0.4274103   0.89  986    10 18619  736
 [91,] 0.96319591   0.03680409   0.5708479   0.9994632 0.4291525   0.90  983    10 18619  739
 [92,] 0.96314677   0.03685323   0.5696864   0.9995169 0.4303139   0.91  981     9 18620  741
 [93,] 0.96309764   0.03690236   0.5685250   0.9995706 0.4314752   0.92  979     8 18621  743
 [94,] 0.96299936   0.03700064   0.5667828   0.9996242 0.4332174   0.93  976     7 18622  746
 [95,] 0.96275367   0.03724633   0.5638792   0.9996242 0.4361210   0.94  971     7 18622  751
 [96,] 0.96250798   0.03749202   0.5598142   0.9997316 0.4401859   0.95  964     5 18624  758
 [97,] 0.96191833   0.03808167   0.5522648   0.9997853 0.4477352   0.96  951     4 18625  771
 [98,] 0.96157437   0.03842563   0.5481998   0.9997853 0.4518003   0.97  944     4 18625  778
 [99,] 0.96113213   0.03886787   0.5418118   0.9998926 0.4581882   0.98  933     2 18627  789
[100,] 0.96073903   0.03926097   0.5365854   0.9999463 0.4634146   0.99  924     1 18628  798
 [ reached 'max' / getOption("max.print") -- omitted 2 rows ]
> 
> #COMPUTING AREA UNDER THE ROC CURVE
> sensitivity<- sensitivity[order(sensitivity)]
> oneminusspec<- oneminusspec[order(oneminusspec)]
> 
> library(Hmisc) #Harrell Miscellaneous packages
> lagx<- Lag(oneminusspec,shift=1)
> lagy<- Lag(sensitivity, shift=1)
> lagx[is.na(lagx)]<- 0
> lagy[is.na(lagy)]<- 0
> trapezoid<- (oneminusspec-lagx)*(sensitivity+lagy)/2
> print(AUC<- sum(trapezoid))
[1] 0.9439808
> 
> 
> #FITTING SVM WITH RADIAL KERNEL
> svm.class<- svm(as.factor(diabetes) ~  gender + age + hypertension + heart_disease + smoking_history + bmi + HbA1c_level + blood_glucose_level, 
+ data=train, kernel="radial")
> 
> #computing prediction accuracy for testing data
> pred.y<- as.numeric(predict(svm.class, test.x))-1
> 
> for (i in 1:length(pred.y))
+   match[i]<- ifelse(test.y[i]==pred.y[i], 1,0)
> print(paste("accuracy=", round(mean(match), digits=4)))
[1] "accuracy= 0.9632"
> 
> 
> #calculating confusion matrix
> tp <- sum(pred.y == 1 & test.y == 1)
> fp <- sum(pred.y == 1 & test.y == 0)
> tn <- sum(pred.y == 0 & test.y == 0)
> fn <- sum(pred.y == 0 & test.y == 1)
> total <- length(test.y)
> 
> # Metrics
> accuracy <- (tp + tn) / total
> misclassrate <- (fp + fn) / total
> sensitivity <- tp / (tp + fn)
> FNR <- fn / (tp + fn)
> specificity <- tn / (fp + tn)
> FPR <- fp / (fp + tn)
> precision <- tp / (tp + fp)
> NPV <- tn / (fn + tn)
> F1score <- 2 * tp / (2 * tp + fn + fp)
> 
> # Print results
> print("Radial Kernel Confusion Matrix Results:")
[1] "Radial Kernel Confusion Matrix Results:"
> print(paste("TP =", tp, " FP =", fp, " TN =", tn, " FN =", fn, "Total =", total))
[1] "TP = 998  FP = 24  TN = 18605  FN = 724 Total = 20351"
> print(paste("Accuracy =", round(accuracy, 4)))
[1] "Accuracy = 0.9632"
> print(paste("MisclassRate =", round(misclassrate, 4)))
[1] "MisclassRate = 0.0368"
> print(paste("Sensitivity =", round(sensitivity, 4)))
[1] "Sensitivity = 0.5796"
> print(paste("FNR =", round(FNR, 4)))
[1] "FNR = 0.4204"
> print(paste("Specificity =", round(specificity, 4)))
[1] "Specificity = 0.9987"
> print(paste("FPR =", round(FPR, 4)))
[1] "FPR = 0.0013"
> print(paste("Precision =", round(precision, 4)))
[1] "Precision = 0.9765"
> print(paste("NPV =", round(NPV, 4)))
[1] "NPV = 0.9625"
> print(paste("F1 Score =", round(F1score, 4)))
[1] "F1 Score = 0.7274"
> 
> ####ROC CURVE CODE
> 
> #COMPUTING CONFUSION MATRICES AND PERFORMANCE MEASURES FOR TESTING SET
> #FOR A RANGE OF CUT-OFFS
> svm.class <- svm(as.factor(diabetes) ~ gender + age + hypertension + heart_disease + smoking_history + bmi + HbA1c_level + blood_glucose_level, data=train, kernel="radial", probability=TRUE)
> 
> 
> svm.prob <- predict(svm.class, newdata=test, probability=TRUE)
> prob_attr <- attr(svm.prob, "probabilities")
> test$prob_1 <- prob_attr[, "1"]
> 
> tpos<- matrix(NA, nrow=nrow(test), ncol=102)
> fpos<- matrix(NA, nrow=nrow(test), ncol=102)
> tneg<- matrix(NA, nrow=nrow(test), ncol=102)
> fneg<- matrix(NA, nrow=nrow(test), ncol=102)
> 
> 
> for (i in 0:101) {
+   tpos[,i+1]<- ifelse(test$diabetes==1 & test$prob_1>=0.01*i,1,0)
+   fpos[,i+1]<- ifelse(test$diabetes==0 & test$prob_1>=0.01*i, 1,0)
+   tneg[,i+1]<- ifelse(test$diabetes==0 & test$prob_1<0.01*i,1,0)
+   fneg[,i+1]<- ifelse(test$diabetes==1 & test$prob_1<0.01*i,1,0)
+ }
> 
> tp<- c()
> fp<- c()
> tn<- c()
> fn<- c()
> accuracy<- c()
> misclassrate<- c()
> sensitivity<- c()
> specificity<- c()
> oneminusspec<- c()
> cutoff<- c()
> 
> 
> for (i in 1:102) {
+   tp[i]<- sum(tpos[,i])
+   fp[i]<- sum(fpos[,i])
+   tn[i]<- sum(tneg[,i])
+   fn[i]<- sum(fneg[,i])
+   total<- nrow(test)
+   accuracy[i]<- (tp[i]+tn[i])/total
+   misclassrate[i]<- (fp[i]+fn[i])/total
+   sensitivity[i]<- tp[i]/(tp[i]+fn[i])
+   specificity[i]<- tn[i]/(fp[i]+tn[i])
+   oneminusspec[i]<- fp[i]/(fp[i]+tn[i])
+   cutoff[i]<- 0.01*(i-1)
+ }
> 
> #PLOTTING ROC CURVE
> plot(oneminusspec, sensitivity, type="l", lty=1, main="The Receiver 
+ Operating Characteristic Curve", xlab="1-Specificity", ylab="Sensitivity")
> points(oneminusspec, sensitivity, pch=0) #pch=plot character, 0=square
> 
> #REPORTING MEASURES FOR THE POINT ON ROC CURVE CLOSEST TO THE IDEAL POINT (0,1)
> distance<- c()
> for (i in 1:102)
+   distance[i]<- sqrt(oneminusspec[i]^2+(1-sensitivity[i])^2)
> 
> measures<- cbind(accuracy, misclassrate, sensitivity, specificity, distance, cutoff, tp, fp, tn, fn)
> min.dist<- min(distance)
> print(measures)
         accuracy misclassrate sensitivity specificity  distance cutoff   tp    fp    tn   fn
  [1,] 0.08461501   0.91538499   1.0000000   0.0000000 1.0000000   0.00 1722 18629     0    0
  [2,] 0.40892339   0.59107661   0.9976771   0.3545010 0.6455031   0.01 1718 12025  6604    4
  [3,] 0.48346519   0.51653481   0.9849013   0.4371142 0.5630883   0.02 1696 10486  8143   26
  [4,] 0.58689991   0.41310009   0.9349593   0.5547265 0.4499986   0.03 1610  8295 10334  112
  [5,] 0.72399391   0.27600609   0.8774681   0.7098073 0.3150014   0.04 1511  5406 13223  211
  [6,] 0.83514324   0.16485676   0.8327526   0.8353642 0.2346841   0.05 1434  3067 15562  288
  [7,] 0.90668763   0.09331237   0.7874564   0.9177089 0.2279179   0.06 1356  1533 17096  366
  [8,] 0.94840548   0.05159452   0.7636469   0.9654839 0.2388601   0.07 1315   643 17986  407
  [9,] 0.95444941   0.04555059   0.7520325   0.9731601 0.2494158   0.08 1295   500 18129  427
 [10,] 0.95788905   0.04211095   0.7427410   0.9777766 0.2582171   0.09 1279   414 18215  443
 [11,] 0.96005110   0.03994890   0.7386760   0.9805143 0.2620495   0.10 1272   363 18266  450
 [12,] 0.96113213   0.03886787   0.7322880   0.9822857 0.2682974   0.11 1261   330 18299  461
 [13,] 0.96226230   0.03773770   0.7235772   0.9843255 0.2768668   0.12 1246   292 18337  476
 [14,] 0.96309764   0.03690236   0.7183508   0.9857212 0.2820110   0.13 1237   266 18363  485
 [15,] 0.96383470   0.03616530   0.7160279   0.9867411 0.2842815   0.14 1233   247 18382  489
 [16,] 0.96462090   0.03537910   0.7119628   0.9879757 0.2882880   0.15 1226   224 18405  496
 [17,] 0.96501400   0.03498600   0.7073171   0.9888346 0.2928958   0.16 1218   208 18421  504
 [18,] 0.96511228   0.03488772   0.7026713   0.9893714 0.2975186   0.17 1210   198 18431  512
 [19,] 0.96530883   0.03469117   0.6991870   0.9899082 0.3009822   0.18 1204   188 18441  518
 [20,] 0.96555452   0.03444548   0.6962834   0.9904450 0.3038669   0.19 1199   178 18451  523
 [21,] 0.96565279   0.03434721   0.6933798   0.9908208 0.3067576   0.20 1194   171 18458  528
 [22,] 0.96560366   0.03439634   0.6898955   0.9910892 0.3102325   0.21 1188   166 18463  534
 [23,] 0.96599676   0.03400324   0.6887340   0.9916260 0.3113786   0.22 1186   156 18473  536
 [24,] 0.96594762   0.03405238   0.6864111   0.9917870 0.3136964   0.23 1182   153 18476  540
 [25,] 0.96604589   0.03395411   0.6846690   0.9920554 0.3154311   0.24 1179   148 18481  543
 [26,] 0.96634072   0.03365928   0.6840883   0.9924312 0.3160024   0.25 1178   141 18488  544
 [27,] 0.96658641   0.03341359   0.6829268   0.9928069 0.3171548   0.26 1176   134 18495  546
 [28,] 0.96658641   0.03341359   0.6817654   0.9929143 0.3183135   0.27 1174   132 18497  548
 [29,] 0.96668468   0.03331532   0.6806039   0.9931290 0.3194699   0.28 1172   128 18501  550
 [30,] 0.96683210   0.03316790   0.6800232   0.9933437 0.3200460   0.29 1171   124 18505  551
 [31,] 0.96683210   0.03316790   0.6782811   0.9935048 0.3217845   0.30 1168   121 18508  554
 [32,] 0.96688123   0.03311877   0.6765389   0.9937195 0.3235221   0.31 1165   117 18512  557
 [33,] 0.96697951   0.03302049   0.6747967   0.9939879 0.3252588   0.32 1162   112 18517  560
 [34,] 0.96702865   0.03297135   0.6730546   0.9942026 0.3269968   0.33 1159   108 18521  563
 [35,] 0.96717606   0.03282394   0.6724739   0.9944173 0.3275737   0.34 1158   104 18525  564
 [36,] 0.96707778   0.03292222   0.6695703   0.9945783 0.3304742   0.35 1153   101 18528  569
 [37,] 0.96717606   0.03282394   0.6689895   0.9947394 0.3310523   0.36 1152    98 18531  570
 [38,] 0.96712692   0.03287308   0.6678281   0.9947931 0.3322127   0.37 1150    97 18532  572
 [39,] 0.96727434   0.03272566   0.6672474   0.9950078 0.3327901   0.38 1149    93 18536  573
 [40,] 0.96717606   0.03282394   0.6643438   0.9951688 0.3356910   0.39 1144    90 18539  578
 [41,] 0.96742175   0.03257825   0.6626016   0.9955983 0.3374271   0.40 1141    82 18547  581
 [42,] 0.96742175   0.03257825   0.6614402   0.9957056 0.3385870   0.41 1139    80 18549  583
 [43,] 0.96747089   0.03252911   0.6602787   0.9958667 0.3397464   0.42 1137    77 18552  585
 [44,] 0.96752002   0.03247998   0.6585366   0.9960814 0.3414859   0.43 1134    73 18556  588
 [45,] 0.96747089   0.03252911   0.6567944   0.9961887 0.3432267   0.44 1131    71 18558  591
 [46,] 0.96756916   0.03243084   0.6567944   0.9962961 0.3432256   0.45 1131    69 18560  591
 [47,] 0.96756916   0.03243084   0.6556330   0.9964035 0.3443858   0.46 1129    67 18562  593
 [48,] 0.96737261   0.03262739   0.6527294   0.9964571 0.3472887   0.47 1124    66 18563  598
 [49,] 0.96727434   0.03272566   0.6515679   0.9964571 0.3484501   0.48 1122    66 18563  600
 [50,] 0.96707778   0.03292222   0.6486643   0.9965108 0.3513530   0.49 1117    65 18564  605
 [51,] 0.96688123   0.03311877   0.6463415   0.9965108 0.3536757   0.50 1113    65 18564  609
 [52,] 0.96668468   0.03331532   0.6434379   0.9965645 0.3565787   0.51 1108    64 18565  614
 [53,] 0.96663555   0.03336445   0.6428571   0.9965645 0.3571594   0.52 1107    64 18565  615
 [54,] 0.96643900   0.03356100   0.6399535   0.9966182 0.3600623   0.53 1102    63 18566  620
 [55,] 0.96643900   0.03356100   0.6387921   0.9967255 0.3612227   0.54 1100    61 18568  622
 [56,] 0.96653727   0.03346273   0.6387921   0.9968329 0.3612218   0.55 1100    59 18570  622
 [57,] 0.96648813   0.03351187   0.6370499   0.9969403 0.3629630   0.56 1097    57 18572  625
 [58,] 0.96634072   0.03365928   0.6353078   0.9969403 0.3647051   0.57 1094    57 18572  628
 [59,] 0.96634072   0.03365928   0.6347271   0.9969939 0.3652853   0.58 1093    56 18573  629
 [60,] 0.96634072   0.03365928   0.6341463   0.9970476 0.3658656   0.59 1092    55 18574  630
 [61,] 0.96629158   0.03370842   0.6329849   0.9971013 0.3670265   0.60 1090    54 18575  632
 [62,] 0.96614417   0.03385583   0.6306620   0.9971550 0.3693489   0.61 1086    53 18576  636
 [63,] 0.96604589   0.03395411   0.6289199   0.9972087 0.3710906   0.62 1083    52 18577  639
 [64,] 0.96575107   0.03424893   0.6254355   0.9972087 0.3745749   0.63 1077    52 18577  645
 [65,] 0.96575107   0.03424893   0.6231127   0.9974234 0.3768961   0.64 1073    48 18581  649
 [66,] 0.96555452   0.03444548   0.6207898   0.9974234 0.3792190   0.65 1069    48 18581  653
 [67,] 0.96540711   0.03459289   0.6190476   0.9974234 0.3809611   0.66 1066    48 18581  656
 [68,] 0.96530883   0.03469117   0.6173055   0.9974771 0.3827029   0.67 1063    47 18582  659
 [69,] 0.96545624   0.03454376   0.6167247   0.9976918 0.3832822   0.68 1062    43 18586  660
 [70,] 0.96516142   0.03483858   0.6132404   0.9976918 0.3867665   0.69 1056    43 18586  666
 [71,] 0.96516142   0.03483858   0.6126597   0.9977455 0.3873469   0.70 1055    42 18587  667
 [72,] 0.96511228   0.03488772   0.6109175   0.9978528 0.3890884   0.71 1052    40 18589  670
 [73,] 0.96476832   0.03523168   0.6068525   0.9978528 0.3931534   0.72 1045    40 18589  677
 [74,] 0.96467004   0.03532996   0.6051103   0.9979065 0.3948952   0.73 1042    39 18590  680
 [75,] 0.96452263   0.03547737   0.6033682   0.9979065 0.3966373   0.74 1039    39 18590  683
 [76,] 0.96447349   0.03552651   0.6016260   0.9980138 0.3983789   0.75 1036    37 18592  686
 [77,] 0.96442435   0.03557565   0.6010453   0.9980138 0.3989596   0.76 1035    37 18592  687
 [78,] 0.96432608   0.03567392   0.5998839   0.9980138 0.4001211   0.77 1033    37 18592  689
 [79,] 0.96422780   0.03577220   0.5987224   0.9980138 0.4012825   0.78 1031    37 18592  691
 [80,] 0.96403125   0.03596875   0.5963995   0.9980138 0.4036054   0.79 1027    37 18592  695
 [81,] 0.96368729   0.03631271   0.5923345   0.9980138 0.4076703   0.80 1020    37 18592  702
 [82,] 0.96344160   0.03655840   0.5894309   0.9980138 0.4105739   0.81 1015    37 18592  707
 [83,] 0.96334332   0.03665668   0.5871080   0.9981212 0.4128963   0.82 1011    35 18594  711
 [84,] 0.96339246   0.03660754   0.5871080   0.9981749 0.4128960   0.83 1011    34 18595  711
 [85,] 0.96319591   0.03680409   0.5836237   0.9982822 0.4163798   0.84 1005    32 18597  717
 [86,] 0.96319591   0.03680409   0.5824623   0.9983896 0.4175409   0.85 1003    30 18599  719
 [87,] 0.96309764   0.03690236   0.5795587   0.9985506 0.4204438   0.86  998    27 18602  724
 [88,] 0.96319591   0.03680409   0.5789779   0.9987117 0.4210240   0.87  997    24 18605  725
 [89,] 0.96309764   0.03690236   0.5766551   0.9988190 0.4233466   0.88  993    22 18607  729
 [90,] 0.96295022   0.03704978   0.5749129   0.9988190 0.4250887   0.89  990    22 18607  732
 [91,] 0.96280281   0.03719719   0.5731707   0.9988190 0.4268309   0.90  987    22 18607  735
 [92,] 0.96265540   0.03734460   0.5708479   0.9988727 0.4291536   0.91  983    21 18608  739
 [93,] 0.96245885   0.03754115   0.5673635   0.9989801 0.4326377   0.92  977    19 18610  745
 [94,] 0.96240971   0.03759029   0.5656214   0.9990874 0.4343796   0.93  974    17 18612  748
 [95,] 0.96216402   0.03783598   0.5627178   0.9990874 0.4372832   0.94  969    17 18612  753
 [96,] 0.96191833   0.03808167   0.5586527   0.9991948 0.4413480   0.95  962    15 18614  760
 [97,] 0.96172178   0.03827822   0.5545877   0.9993558 0.4454128   0.96  955    12 18617  767
 [98,] 0.96142696   0.03857304   0.5493612   0.9995169 0.4506391   0.97  946     9 18620  776
 [99,] 0.96073903   0.03926097   0.5400697   0.9996242 0.4599305   0.98  930     7 18622  792
[100,] 0.95990369   0.04009631   0.5296167   0.9996779 0.4703834   0.99  912     6 18623  810
 [ reached 'max' / getOption("max.print") -- omitted 2 rows ]
> 
> #COMPUTING AREA UNDER THE ROC CURVE
> sensitivity<- sensitivity[order(sensitivity)]
> oneminusspec<- oneminusspec[order(oneminusspec)]
> 
> library(Hmisc) #Harrell Miscellaneous packages
> lagx<- Lag(oneminusspec,shift=1)
> lagy<- Lag(sensitivity, shift=1)
> lagx[is.na(lagx)]<- 0
> lagy[is.na(lagy)]<- 0
> trapezoid<- (oneminusspec-lagx)*(sensitivity+lagy)/2
> print(AUC<- sum(trapezoid))
[1] 0.9250619
> 
> 
> #FITTING SVM WITH SIGMOID KERNEL
> svm.class<- svm(as.factor(diabetes) ~  gender + age + hypertension + heart_disease + smoking_history + bmi + HbA1c_level + blood_glucose_level, 
+ data=train, kernel="sigmoid")
> 
> #computing prediction accuracy for testing data
> pred.y<- as.numeric(predict(svm.class, test.x))-1
> 
> for (i in 1:length(pred.y))
+   match[i]<- ifelse(test.y[i]==pred.y[i], 1,0)
> print(paste("accuracy=", round(mean(match), digits=4)))
[1] "accuracy= 0.9143"
> 
> 
> #calculating confusion matrix
> tp <- sum(pred.y == 1 & test.y == 1)
> fp <- sum(pred.y == 1 & test.y == 0)
> tn <- sum(pred.y == 0 & test.y == 0)
> fn <- sum(pred.y == 0 & test.y == 1)
> total <- length(test.y)
> 
> # Metrics
> accuracy <- (tp + tn) / total
> misclassrate <- (fp + fn) / total
> sensitivity <- tp / (tp + fn)
> FNR <- fn / (tp + fn)
> specificity <- tn / (fp + tn)
> FPR <- fp / (fp + tn)
> precision <- tp / (tp + fp)
> NPV <- tn / (fn + tn)
> F1score <- 2 * tp / (2 * tp + fn + fp)
> 
> # Print results
> print("Sigmoid Kernel Confusion Matrix Results:")
[1] "Sigmoid Kernel Confusion Matrix Results:"
> print(paste("TP =", tp, " FP =", fp, " TN =", tn, " FN =", fn, "Total =", total))
[1] "TP = 838  FP = 861  TN = 17768  FN = 884 Total = 20351"
> print(paste("Accuracy =", round(accuracy, 4)))
[1] "Accuracy = 0.9143"
> print(paste("MisclassRate =", round(misclassrate, 4)))
[1] "MisclassRate = 0.0857"
> print(paste("Sensitivity =", round(sensitivity, 4)))
[1] "Sensitivity = 0.4866"
> print(paste("FNR =", round(FNR, 4)))
[1] "FNR = 0.5134"
> print(paste("Specificity =", round(specificity, 4)))
[1] "Specificity = 0.9538"
> print(paste("FPR =", round(FPR, 4)))
[1] "FPR = 0.0462"
> print(paste("Precision =", round(precision, 4)))
[1] "Precision = 0.4932"
> print(paste("NPV =", round(NPV, 4)))
[1] "NPV = 0.9526"
> print(paste("F1 Score =", round(F1score, 4)))
[1] "F1 Score = 0.4899"
> 
> ####ROC CURVE CODE
> 
> #COMPUTING CONFUSION MATRICES AND PERFORMANCE MEASURES FOR TESTING SET
> #FOR A RANGE OF CUT-OFFS
> svm.class <- svm(as.factor(diabetes) ~ gender + age + hypertension + heart_disease + smoking_history + bmi + HbA1c_level + blood_glucose_level, data=train, kernel="sigmoid", probability=TRUE)
> 
> svm.prob <- predict(svm.class, newdata=test, probability=TRUE)
> prob_attr <- attr(svm.prob, "probabilities")
> test$prob_1 <- prob_attr[, "1"]
> 
> tpos<- matrix(NA, nrow=nrow(test), ncol=102)
> fpos<- matrix(NA, nrow=nrow(test), ncol=102)
> tneg<- matrix(NA, nrow=nrow(test), ncol=102)
> fneg<- matrix(NA, nrow=nrow(test), ncol=102)
> 
> 
> for (i in 0:101) {
+   tpos[,i+1]<- ifelse(test$diabetes==1 & test$prob_1>=0.01*i,1,0)
+   fpos[,i+1]<- ifelse(test$diabetes==0 & test$prob_1>=0.01*i, 1,0)
+   tneg[,i+1]<- ifelse(test$diabetes==0 & test$prob_1<0.01*i,1,0)
+   fneg[,i+1]<- ifelse(test$diabetes==1 & test$prob_1<0.01*i,1,0)
+ }
> 
> tp<- c()
> fp<- c()
> tn<- c()
> fn<- c()
> accuracy<- c()
> misclassrate<- c()
> sensitivity<- c()
> specificity<- c()
> oneminusspec<- c()
> cutoff<- c()
> 
> 
> for (i in 1:102) {
+   tp[i]<- sum(tpos[,i])
+   fp[i]<- sum(fpos[,i])
+   tn[i]<- sum(tneg[,i])
+   fn[i]<- sum(fneg[,i])
+   total<- nrow(test)
+   accuracy[i]<- (tp[i]+tn[i])/total
+   misclassrate[i]<- (fp[i]+fn[i])/total
+   sensitivity[i]<- tp[i]/(tp[i]+fn[i])
+   specificity[i]<- tn[i]/(fp[i]+tn[i])
+   oneminusspec[i]<- fp[i]/(fp[i]+tn[i])
+   cutoff[i]<- 0.01*(i-1)
+ }
> 
> #PLOTTING ROC CURVE
> plot(oneminusspec, sensitivity, type="l", lty=1, main="The Receiver 
+ Operating Characteristic Curve", xlab="1-Specificity", ylab="Sensitivity")
> points(oneminusspec, sensitivity, pch=0) #pch=plot character, 0=square
> 
> #REPORTING MEASURES FOR THE POINT ON ROC CURVE CLOSEST TO THE IDEAL POINT (0,1)
> distance<- c()
> for (i in 1:102)
+   distance[i]<- sqrt(oneminusspec[i]^2+(1-sensitivity[i])^2)
> 
> measures<- cbind(accuracy, misclassrate, sensitivity, specificity, distance, cutoff, tp, fp, tn, fn)
> min.dist<- min(distance)
> print(measures)
         accuracy misclassrate  sensitivity specificity  distance cutoff   tp    fp    tn   fn
  [1,] 0.08461501   0.91538499 1.0000000000   0.0000000 1.0000000   0.00 1722 18629     0    0
  [2,] 0.41806299   0.58193701 0.9564459930   0.3682967 0.6332029   0.01 1647 11768  6861   75
  [3,] 0.56621296   0.43378704 0.9210220674   0.5334156 0.4732214   0.02 1586  8692  9937  136
  [4,] 0.64763402   0.35236598 0.8670150987   0.6273552 0.3956629   0.03 1493  6942 11687  229
  [5,] 0.70193111   0.29806889 0.8199767712   0.6910194 0.3575995   0.04 1412  5756 12873  310
  [6,] 0.73917744   0.26082256 0.7950058072   0.7340169 0.3358119   0.05 1369  4955 13674  353
  [7,] 0.76782468   0.23217532 0.7694541231   0.7676741 0.3273022   0.06 1325  4328 14301  397
  [8,] 0.78978920   0.21021080 0.7409988386   0.7942992 0.3307483   0.07 1276  3832 14797  446
  [9,] 0.80988649   0.19011351 0.7090592334   0.8192066 0.3425387   0.08 1221  3368 15261  501
 [10,] 0.82698639   0.17301361 0.6898954704   0.8396586 0.3491048   0.09 1188  2987 15642  534
 [11,] 0.84030269   0.15969731 0.6713124274   0.8559236 0.3588782   0.10 1156  2684 15945  566
 [12,] 0.85248882   0.14751118 0.6527293844   0.8709539 0.3704724   0.11 1124  2404 16225  598
 [13,] 0.86187411   0.13812589 0.6335656214   0.8829782 0.3846664   0.12 1091  2180 16449  631
 [14,] 0.87179991   0.12820009 0.6155632985   0.8954855 0.3983903   0.13 1060  1947 16682  662
 [15,] 0.87971107   0.12028893 0.5975609756   0.9057920 0.4133187   0.14 1029  1755 16874  693
 [16,] 0.88614810   0.11385190 0.5842044135   0.9140587 0.4245844   0.15 1006  1601 17028  716
 [17,] 0.89076704   0.10923296 0.5691056911   0.9205003 0.4381668   0.16  980  1481 17148  742
 [18,] 0.89484546   0.10515454 0.5586527294   0.9259219 0.4475209   0.17  962  1380 17249  760
 [19,] 0.89887475   0.10112525 0.5406504065   0.9319878 0.4643573   0.18  931  1267 17362  791
 [20,] 0.90241266   0.09758734 0.5307781649   0.9367653 0.4734636   0.19  914  1178 17451  808
 [21,] 0.90580315   0.09419685 0.5226480836   0.9412207 0.4809572   0.20  900  1095 17534  822
 [22,] 0.90889883   0.09110117 0.5150987224   0.9453003 0.4879767   0.21  887  1019 17610  835
 [23,] 0.91120829   0.08879171 0.5069686411   0.9485748 0.4957060   0.22  873   958 17671  849
 [24,] 0.91243674   0.08756326 0.4976771196   0.9507757 0.5047289   0.23  857   917 17712  865
 [25,] 0.91400914   0.08599086 0.4901277584   0.9531913 0.5120164   0.24  844   872 17757  878
 [26,] 0.91592551   0.08407449 0.4802555168   0.9561973 0.5215870   0.25  827   816 17813  895
 [27,] 0.91774360   0.08225640 0.4750290360   0.9586666 0.5265956   0.26  818   770 17859  904
 [28,] 0.91897204   0.08102796 0.4639953542   0.9610285 0.5374195   0.27  799   726 17903  923
 [29,] 0.92059358   0.07940642 0.4564459930   0.9634978 0.5447783   0.28  786   680 17949  936
 [30,] 0.92231340   0.07768660 0.4512195122   0.9658597 0.5498414   0.29  777   636 17993  945
 [31,] 0.92359098   0.07640902 0.4454123113   0.9677922 0.5555221   0.30  767   600 18029  955
 [32,] 0.92418063   0.07581937 0.4349593496   0.9694025 0.5658685   0.31  749   570 18059  973
 [33,] 0.92511425   0.07488575 0.4291521487   0.9709593 0.5715861   0.32  739   541 18088  983
 [34,] 0.92545821   0.07454179 0.4192799071   0.9722476 0.5813829   0.33  722   517 18112 1000
 [35,] 0.92599872   0.07400128 0.4099883856   0.9736969 0.5905976   0.34  706   490 18139 1016
 [36,] 0.92653924   0.07346076 0.4024390244   0.9749852 0.5980843   0.35  693   466 18163 1029
 [37,] 0.92649010   0.07350990 0.3914053426   0.9759515 0.6090696   0.36  674   448 18181 1048
 [38,] 0.92619527   0.07380473 0.3809523810   0.9765956 0.6194899   0.37  656   436 18193 1066
 [39,] 0.92590045   0.07409955 0.3658536585   0.9776692 0.6345394   0.38  630   416 18213 1092
 [40,] 0.92599872   0.07400128 0.3554006969   0.9787428 0.6449497   0.39  612   396 18233 1110
 [41,] 0.92565476   0.07434524 0.3461091754   0.9792259 0.6542207   0.40  596   387 18242 1126
 [42,] 0.92565476   0.07434524 0.3391405343   0.9798701 0.6611660   0.41  584   375 18254 1138
 [43,] 0.92516338   0.07483662 0.3252032520   0.9806216 0.6750749   0.42  560   361 18268 1162
 [44,] 0.92481942   0.07518058 0.3124274100   0.9814268 0.6878234   0.43  538   346 18283 1184
 [45,] 0.92437718   0.07562282 0.3008130081   0.9820173 0.6994182   0.44  518   335 18294 1204
 [46,] 0.92373839   0.07626161 0.2880371661   0.9825004 0.7121779   0.45  496   326 18303 1226
 [47,] 0.92324701   0.07675299 0.2781649245   0.9828762 0.7220382   0.46  479   319 18310 1243
 [48,] 0.92319788   0.07680212 0.2711962834   0.9834666 0.7289912   0.47  467   308 18321 1255
 [49,] 0.92250995   0.07749005 0.2584204413   0.9838961 0.7417544   0.48  445   300 18329 1277
 [50,] 0.92192030   0.07807970 0.2502903600   0.9840034 0.7498803   0.49  431   298 18331 1291
 [51,] 0.92177289   0.07822711 0.2439024390   0.9844329 0.7562578   0.50  420   290 18339 1302
 [52,] 0.92162547   0.07837453 0.2369337979   0.9849160 0.7632153   0.51  408   281 18348 1314
 [53,] 0.92147806   0.07852194 0.2299651568   0.9853991 0.7701733   0.52  396   272 18357 1326
 [54,] 0.92064272   0.07935728 0.2177700348   0.9856138 0.7823622   0.53  375   268 18361 1347
 [55,] 0.92054444   0.07945556 0.2131242741   0.9859359 0.7870014   0.54  367   262 18367 1355
 [56,] 0.92024962   0.07975038 0.2067363531   0.9862043 0.7933836   0.55  356   257 18372 1366
 [57,] 0.91990566   0.08009434 0.2003484321   0.9864190 0.7997669   0.56  345   253 18376 1377
 [58,] 0.91936514   0.08063486 0.1933797909   0.9864727 0.8067336   0.57  333   252 18377 1389
 [59,] 0.91877549   0.08122451 0.1846689895   0.9866337 0.8154406   0.58  318   249 18380 1404
 [60,] 0.91818584   0.08181416 0.1765389082   0.9867411 0.8235678   0.59  304   247 18382 1418
 [61,] 0.91808756   0.08191244 0.1718931475   0.9870632 0.8282079   0.60  296   241 18388 1426
 [62,] 0.91749791   0.08250209 0.1649245064   0.9870632 0.8351757   0.61  284   241 18388 1438
 [63,] 0.91700654   0.08299346 0.1579558653   0.9871705 0.8421419   0.62  272   239 18390 1450
 [64,] 0.91641688   0.08358312 0.1498257840   0.9872779 0.8502694   0.63  258   237 18392 1464
 [65,] 0.91577809   0.08422191 0.1416957027   0.9873316 0.8583978   0.64  244   236 18393 1478
 [66,] 0.91538499   0.08461501 0.1353077816   0.9874926 0.8647827   0.65  233   233 18396 1489
 [67,] 0.91528672   0.08471328 0.1306620209   0.9878147 0.8694234   0.66  225   227 18402 1497
 [68,] 0.91523758   0.08476242 0.1271777003   0.9880831 0.8729036   0.67  219   222 18407 1503
 [69,] 0.91494275   0.08505725 0.1213704994   0.9882978 0.8787074   0.68  209   218 18411 1513
 [70,] 0.91464793   0.08535207 0.1161440186   0.9884589 0.8839313   0.69  200   215 18414 1522
 [71,] 0.91445138   0.08554862 0.1109175377   0.9887273 0.8891539   0.70  191   210 18419 1531
 [72,] 0.91445138   0.08554862 0.1056910569   0.9892104 0.8943740   0.71  182   201 18428 1540
 [73,] 0.91445138   0.08554862 0.0993031359   0.9898008 0.9007546   0.72  171   190 18439 1551
 [74,] 0.91405828   0.08594172 0.0905923345   0.9901766 0.9094607   0.73  156   183 18446 1566
 [75,] 0.91440224   0.08559776 0.0853658537   0.9910355 0.9146781   0.74  147   167 18462 1575
 [76,] 0.91445138   0.08554862 0.0766550523   0.9918944 0.9233805   0.75  132   151 18478 1590
 [77,] 0.91430397   0.08569603 0.0685249710   0.9924848 0.9315053   0.76  118   140 18489 1604
 [78,] 0.91430397   0.08569603 0.0621370499   0.9930753 0.9378885   0.77  107   129 18500 1615
 [79,] 0.91405828   0.08594172 0.0481997677   0.9940952 0.9518185   0.78   83   110 18519 1639
 [80,] 0.91425483   0.08574517 0.0400696864   0.9950615 0.9599430   0.79   69    92 18537 1653
 [81,] 0.91450052   0.08549948 0.0331010453   0.9959740 0.9669073   0.80   57    75 18554 1665
 [82,] 0.91435310   0.08564690 0.0249709640   0.9965645 0.9750351   0.81   43    64 18565 1679
 [83,] 0.91430397   0.08569603 0.0185830430   0.9971013 0.9814212   0.82   32    54 18575 1690
 [84,] 0.91474620   0.08525380 0.0127758420   0.9981212 0.9872259   0.83   22    35 18594 1700
 [85,] 0.91459879   0.08540121 0.0052264808   0.9986580 0.9947744   0.84    9    25 18604 1713
 [86,] 0.91464793   0.08535207 0.0011614402   0.9990874 0.9988390   0.85    2    17 18612 1720
 [87,] 0.91484448   0.08515552 0.0005807201   0.9993558 0.9994195   0.86    1    12 18617 1721
 [88,] 0.91494275   0.08505725 0.0000000000   0.9995169 1.0000001   0.87    0     9 18620 1722
 [89,] 0.91528672   0.08471328 0.0000000000   0.9998926 1.0000000   0.88    0     2 18627 1722
 [90,] 0.91538499   0.08461501 0.0000000000   1.0000000 1.0000000   0.89    0     0 18629 1722
 [91,] 0.91538499   0.08461501 0.0000000000   1.0000000 1.0000000   0.90    0     0 18629 1722
 [92,] 0.91538499   0.08461501 0.0000000000   1.0000000 1.0000000   0.91    0     0 18629 1722
 [93,] 0.91538499   0.08461501 0.0000000000   1.0000000 1.0000000   0.92    0     0 18629 1722
 [94,] 0.91538499   0.08461501 0.0000000000   1.0000000 1.0000000   0.93    0     0 18629 1722
 [95,] 0.91538499   0.08461501 0.0000000000   1.0000000 1.0000000   0.94    0     0 18629 1722
 [96,] 0.91538499   0.08461501 0.0000000000   1.0000000 1.0000000   0.95    0     0 18629 1722
 [97,] 0.91538499   0.08461501 0.0000000000   1.0000000 1.0000000   0.96    0     0 18629 1722
 [98,] 0.91538499   0.08461501 0.0000000000   1.0000000 1.0000000   0.97    0     0 18629 1722
 [99,] 0.91538499   0.08461501 0.0000000000   1.0000000 1.0000000   0.98    0     0 18629 1722
[100,] 0.91538499   0.08461501 0.0000000000   1.0000000 1.0000000   0.99    0     0 18629 1722
 [ reached 'max' / getOption("max.print") -- omitted 2 rows ]
> 
> #COMPUTING AREA UNDER THE ROC CURVE
> sensitivity<- sensitivity[order(sensitivity)]
> oneminusspec<- oneminusspec[order(oneminusspec)]
> 
> library(Hmisc) #Harrell Miscellaneous packages
> lagx<- Lag(oneminusspec,shift=1)
> lagy<- Lag(sensitivity, shift=1)
> lagx[is.na(lagx)]<- 0
> lagy[is.na(lagy)]<- 0
> trapezoid<- (oneminusspec-lagx)*(sensitivity+lagy)/2
> print(AUC<- sum(trapezoid))
[1] 0.8496547