diabetes.data<- read.csv(file="C:/Users/sathv/OneDrive/Stats Research Project/diabetes_prediction_dataset.csv", 
+ header=TRUE, sep=",")
> 
> diabetes.cat<- ifelse(diabetes.data$diabetes==1,1,0)
> diabetes.data$smoking_history<- ifelse(diabetes.data$smoking_history %in% c("current", "former"),1,0)  
> diabetes.data$gender<- ifelse(diabetes.data$gender=='Male',1,0)           
> 
> class0 <- subset(diabetes.data, diabetes == 0)
> class1 <- subset(diabetes.data, diabetes == 1)
> 
> set.seed(42)
> class0_sample <- class0[sample(nrow(class0), size = 4 * nrow(class1)), ]
> balanced_data <- rbind(class0_sample, class1)
> 
> library(caret)
> 
> set.seed(503548)
> # Stratified partitioning: ensures train/test have similar class balance
> split_index <- createDataPartition(balanced_data$diabetes, p=0.8, list=FALSE)
> train <- balanced_data[split_index, ]
> test <- balanced_data[-split_index, ]
> 
> train$diabetes <- as.numeric(train$diabetes)
> test$diabetes <- as.numeric(test$diabetes)
> 
> train.x<- data.matrix(train[-9])
> train.y<- data.matrix(train[9])
> test.x<- data.matrix(test[-9])
> test.y<- data.matrix(test[9])
> 
> library(neuralnet)
> 
> #FITTING ANN WITH LOGISTIC ACTIVATION FUNCTION AND ONE LAYER WITH THREE NEURONS
> ann.log.class<- neuralnet(diabetes ~ gender + age + hypertension + heart_disease + smoking_history + bmi + HbA1c_level + blood_glucose_level, 
+                                    data=train, hidden = 3, act.fct="logistic", stepmax=1e7)
> 
> #PLOTTING THE DIAGRAM
> plot(ann.log.class)
> 
> #COMPUTING PREDICTION ACCURACY FOR TESTING DATA
> pred.prob<- predict(ann.log.class, test.x)[,1]
> 
> pred.y<- c()
> match<- c()
> for (i in 1:length(test.y)){
+   pred.y[i]<- ifelse(pred.prob[i]>0.5,1,0)
+   match[i]<- ifelse(test.y[i]==pred.y[i],1,0)
+ }
> 
> print(paste("accuracy=", round(mean(match), digits=4)))
[1] "accuracy= 0.9212"
> 
> 
> #calculating confusion matrix
> tp <- sum(pred.y == 1 & test.y == 1)
> fp <- sum(pred.y == 1 & test.y == 0)
> tn <- sum(pred.y == 0 & test.y == 0)
> fn <- sum(pred.y == 0 & test.y == 1)
> total <- length(test.y)
> 
> # Metrics
> accuracy <- (tp + tn) / total
> misclassrate <- (fp + fn) / total
> sensitivity <- tp / (tp + fn)
> FNR <- fn / (tp + fn)
> specificity <- tn / (fp + tn)
> FPR <- fp / (fp + tn)
> precision <- tp / (tp + fp)
> NPV <- tn / (fn + tn)
> F1score <- 2 * tp / (2 * tp + fn + fp)
> 
> # Print results
> print("ANN Confusion Matrix Results:")
[1] "ANN Confusion Matrix Results:"
> print(paste("TP =", tp, " FP =", fp, " TN =", tn, " FN =", fn, "Total =", total))
[1] "TP = 1237  FP = 173  TN = 6593  FN = 497 Total = 8500"
> print(paste("Accuracy =", round(accuracy, 4)))
[1] "Accuracy = 0.9212"
> print(paste("MisclassRate =", round(misclassrate, 4)))
[1] "MisclassRate = 0.0788"
> print(paste("Sensitivity =", round(sensitivity, 4)))
[1] "Sensitivity = 0.7134"
> print(paste("FNR =", round(FNR, 4)))
[1] "FNR = 0.2866"
> print(paste("Specificity =", round(specificity, 4)))
[1] "Specificity = 0.9744"
> print(paste("FPR =", round(FPR, 4)))
[1] "FPR = 0.0256"
> print(paste("Precision =", round(precision, 4)))
[1] "Precision = 0.8773"
> print(paste("NPV =", round(NPV, 4)))
[1] "NPV = 0.9299"
> print(paste("F1 Score =", round(F1score, 4)))
[1] "F1 Score = 0.7869"
> 
> 
> ####ROC CURVE CODE
> 
> #COMPUTING CONFUSION MATRICES AND PERFORMANCE MEASURES FOR TESTING SET
> #FOR A RANGE OF CUT-OFFS
> 
> tpos<- matrix(NA, nrow=nrow(test), ncol=102)
> fpos<- matrix(NA, nrow=nrow(test), ncol=102)
> tneg<- matrix(NA, nrow=nrow(test), ncol=102)
> fneg<- matrix(NA, nrow=nrow(test), ncol=102)
> 
> 
> for (i in 0:101) {
+   tpos[,i+1]<- ifelse(test.y==1 & pred.prob>=0.01*i,1,0)
+   fpos[,i+1]<- ifelse(test.y==0 & pred.prob>=0.01*i, 1,0)
+   tneg[,i+1]<- ifelse(test.y==0 & pred.prob<0.01*i,1,0)
+   fneg[,i+1]<- ifelse(test.y==1 & pred.prob<0.01*i,1,0)
+ }
> 
> tp<- c()
> fp<- c()
> tn<- c()
> fn<- c()
> accuracy<- c()
> misclassrate<- c()
> sensitivity<- c()
> specificity<- c()
> oneminusspec<- c()
> cutoff<- c()
> 
> 
> for (i in 1:102) {
+   tp[i]<- sum(tpos[,i])
+   fp[i]<- sum(fpos[,i])
+   tn[i]<- sum(tneg[,i])
+   fn[i]<- sum(fneg[,i])
+   total<- nrow(test)
+   accuracy[i]<- (tp[i]+tn[i])/total
+   misclassrate[i]<- (fp[i]+fn[i])/total
+   sensitivity[i]<- tp[i]/(tp[i]+fn[i])
+   specificity[i]<- tn[i]/(fp[i]+tn[i])
+   oneminusspec[i]<- fp[i]/(fp[i]+tn[i])
+   cutoff[i]<- 0.01*(i-1)
+ }
> 
> #PLOTTING ROC CURVE
> plot(oneminusspec, sensitivity, type="l", lty=1, main="The Receiver 
+ Operating Characteristic Curve", xlab="1-Specificity", ylab="Sensitivity")
> points(oneminusspec, sensitivity, pch=0) #pch=plot character, 0=square
> 
> #REPORTING MEASURES FOR THE POINT ON ROC CURVE CLOSEST TO THE IDEAL POINT (0,1)
> distance<- c()
> for (i in 1:102)
+   distance[i]<- sqrt(oneminusspec[i]^2+(1-sensitivity[i])^2)
> 
> measures<- cbind(accuracy, misclassrate, sensitivity, specificity, distance, cutoff, tp, fp, tn, fn)
> min.dist<- min(distance)
> print(measures)
        accuracy misclassrate sensitivity specificity  distance cutoff   tp   fp   tn   fn
  [1,] 0.3565882   0.64341176   1.0000000   0.1916938 0.8083062   0.00 1734 5469 1297    0
  [2,] 0.5701176   0.42988235   0.9982699   0.4603902 0.5396126   0.01 1731 3651 3115    3
  [3,] 0.6454118   0.35458824   0.9953864   0.5557198 0.4443042   0.02 1726 3006 3760    8
  [4,] 0.6895294   0.31047059   0.9913495   0.6121785 0.3879179   0.03 1719 2624 4142   15
  [5,] 0.7203529   0.27964706   0.9855825   0.6523795 0.3479193   0.04 1709 2352 4414   25
  [6,] 0.7482353   0.25176471   0.9798155   0.6888856 0.3117685   0.05 1699 2105 4661   35
  [7,] 0.7711765   0.22882353   0.9752018   0.7188886 0.2822031   0.06 1691 1902 4864   43
  [8,] 0.7912941   0.20870588   0.9682814   0.7459356 0.2560367   0.07 1679 1719 5047   55
  [9,] 0.8029412   0.19705882   0.9596309   0.7627845 0.2406260   0.08 1664 1605 5161   70
 [10,] 0.8145882   0.18541176   0.9515571   0.7794857 0.2257726   0.09 1650 1492 5274   84
 [11,] 0.8249412   0.17505882   0.9457901   0.7939698 0.2130426   0.10 1640 1394 5372   94
 [12,] 0.8331765   0.16682353   0.9382930   0.8062371 0.2033515   0.11 1627 1311 5455  107
 [13,] 0.8423529   0.15764706   0.9267589   0.8207213 0.1936624   0.12 1607 1213 5553  127
 [14,] 0.8507059   0.14929412   0.9227220   0.8322495 0.1846947   0.13 1600 1135 5631  134
 [15,] 0.8565882   0.14341176   0.9192618   0.8405262 0.1787472   0.14 1594 1079 5687  140
 [16,] 0.8632941   0.13670588   0.9140715   0.8502808 0.1726254   0.15 1585 1013 5753  149
 [17,] 0.8684706   0.13152941   0.9083045   0.8582619 0.1688128   0.16 1575  959 5807  159
 [18,] 0.8734118   0.12658824   0.9031142   0.8657996 0.1655192   0.17 1566  908 5858  168
 [19,] 0.8767059   0.12329412   0.8944637   0.8721549 0.1657778   0.18 1551  865 5901  183
 [20,] 0.8796471   0.12035294   0.8858131   0.8780668 0.1670519   0.19 1536  825 5941  198
 [21,] 0.8834118   0.11658824   0.8742791   0.8857523 0.1698772   0.20 1516  773 5993  218
 [22,] 0.8867059   0.11329412   0.8673587   0.8916642 0.1712611   0.21 1504  733 6033  230
 [23,] 0.8900000   0.11000000   0.8610150   0.8974283 0.1727362   0.22 1493  694 6072  241
 [24,] 0.8916471   0.10835294   0.8517878   0.9018623 0.1777579   0.23 1477  664 6102  257
 [25,] 0.8947059   0.10529412   0.8454441   0.9073308 0.1802086   0.24 1466  627 6139  268
 [26,] 0.8975294   0.10247059   0.8379469   0.9127993 0.1840249   0.25 1453  590 6176  281
 [27,] 0.8988235   0.10117647   0.8310265   0.9161986 0.1886126   0.26 1441  567 6199  293
 [28,] 0.9016471   0.09835294   0.8258362   0.9210760 0.1912120   0.27 1432  534 6232  302
 [29,] 0.9043529   0.09564706   0.8229527   0.9252143 0.1921943   0.28 1427  506 6260  307
 [30,] 0.9063529   0.09364706   0.8200692   0.9284659 0.1936291   0.29 1422  484 6282  312
 [31,] 0.9084706   0.09152941   0.8160323   0.9321608 0.1960772   0.30 1415  459 6307  319
 [32,] 0.9092941   0.09070588   0.8091119   0.9349690 0.2016614   0.31 1403  440 6326  331
 [33,] 0.9104706   0.08952941   0.8016148   0.9383683 0.2077382   0.32 1390  417 6349  344
 [34,] 0.9122353   0.08776471   0.7993080   0.9411765 0.2091351   0.33 1386  398 6368  348
 [35,] 0.9147059   0.08529412   0.7970012   0.9448714 0.2103514   0.34 1382  373 6393  352
 [36,] 0.9152941   0.08470588   0.7923875   0.9467928 0.2143220   0.35 1374  360 6406  360
 [37,] 0.9158824   0.08411765   0.7877739   0.9487142 0.2183349   0.36 1366  347 6419  368
 [38,] 0.9169412   0.08305882   0.7837370   0.9510789 0.2217272   0.37 1359  331 6435  375
 [39,] 0.9174118   0.08258824   0.7791234   0.9528525 0.2258525   0.38 1351  319 6447  383
 [40,] 0.9177647   0.08223529   0.7745098   0.9544783 0.2300392   0.39 1343  308 6458  391
 [41,] 0.9182353   0.08176471   0.7675894   0.9568430 0.2363836   0.40 1331  292 6474  403
 [42,] 0.9195294   0.08047059   0.7623991   0.9597990 0.2409778   0.41 1322  272 6494  412
 [43,] 0.9195294   0.08047059   0.7549020   0.9617204 0.2480693   0.42 1309  259 6507  425
 [44,] 0.9198824   0.08011765   0.7497116   0.9634939 0.2529367   0.43 1300  247 6519  434
 [45,] 0.9198824   0.08011765   0.7456747   0.9645285 0.2567870   0.44 1293  240 6526  441
 [46,] 0.9201176   0.07988235   0.7404844   0.9661543 0.2617133   0.45 1284  229 6537  450
 [47,] 0.9202353   0.07976471   0.7329873   0.9682235 0.2688969   0.46 1271  215 6551  463
 [48,] 0.9204706   0.07952941   0.7289504   0.9695537 0.2727542   0.47 1264  206 6560  470
 [49,] 0.9202353   0.07976471   0.7226067   0.9708838 0.2789172   0.48 1253  197 6569  481
 [50,] 0.9208235   0.07917647   0.7191465   0.9725096 0.2821957   0.49 1247  186 6580  487
 [51,] 0.9211765   0.07882353   0.7133795   0.9744310 0.2877588   0.50 1237  173 6593  497
 [52,] 0.9210588   0.07894118   0.7087659   0.9754656 0.2922657   0.51 1229  166 6600  505
 [53,] 0.9211765   0.07882353   0.7047290   0.9766479 0.2961930   0.52 1222  158 6608  512
 [54,] 0.9214118   0.07858824   0.7018454   0.9776825 0.2989886   0.53 1217  151 6615  517
 [55,] 0.9212941   0.07870588   0.6978085   0.9785693 0.3029504   0.54 1210  145 6621  524
 [56,] 0.9211765   0.07882353   0.6908881   0.9801951 0.3097457   0.55 1198  134 6632  536
 [57,] 0.9212941   0.07870588   0.6856978   0.9816731 0.3148361   0.56 1189  124 6642  545
 [58,] 0.9208235   0.07917647   0.6793541   0.9827077 0.3211119   0.57 1178  117 6649  556
 [59,] 0.9207059   0.07929412   0.6764706   0.9832988 0.3239602   0.58 1173  113 6653  561
 [60,] 0.9195294   0.08047059   0.6695502   0.9835944 0.3308568   0.59 1161  111 6655  573
 [61,] 0.9197647   0.08023529   0.6649366   0.9850724 0.3353958   0.60 1153  101 6665  581
 [62,] 0.9202353   0.07976471   0.6608997   0.9866982 0.3393611   0.61 1146   90 6676  588
 [63,] 0.9203529   0.07964706   0.6591696   0.9872894 0.3410674   0.62 1143   86 6680  591
 [64,] 0.9196471   0.08035294   0.6510957   0.9884718 0.3490947   0.63 1129   78 6688  605
 [65,] 0.9189412   0.08105882   0.6441753   0.9893586 0.3559838   0.64 1117   72 6694  617
 [66,] 0.9185882   0.08141176   0.6395617   0.9900975 0.3605743   0.65 1109   67 6699  625
 [67,] 0.9185882   0.08141176   0.6343714   0.9914277 0.3657291   0.66 1100   58 6708  634
 [68,] 0.9175294   0.08247059   0.6268743   0.9920189 0.3732111   0.67 1087   54 6712  647
 [69,] 0.9170588   0.08294118   0.6234141   0.9923145 0.3766643   0.68 1081   52 6714  653
 [70,] 0.9163529   0.08364706   0.6182238   0.9927579 0.3818449   0.69 1072   49 6717  662
 [71,] 0.9154118   0.08458824   0.6113033   0.9933491 0.3887536   0.70 1060   45 6721  674
 [72,] 0.9147059   0.08529412   0.6072664   0.9934969 0.3927874   0.71 1053   44 6722  681
 [73,] 0.9131765   0.08682353   0.5986159   0.9937925 0.4014321   0.72 1038   42 6724  696
 [74,] 0.9130588   0.08694118   0.5957324   0.9943837 0.4043066   0.73 1033   38 6728  701
 [75,] 0.9128235   0.08717647   0.5928489   0.9948271 0.4071840   0.74 1028   35 6731  706
 [76,] 0.9125882   0.08741176   0.5888120   0.9955661 0.4112119   0.75 1021   30 6736  713
 [77,] 0.9120000   0.08800000   0.5841984   0.9960095 0.4158208   0.76 1013   27 6739  721
 [78,] 0.9114118   0.08858824   0.5784314   0.9967484 0.4215812   0.77 1003   22 6744  731
 [79,] 0.9101176   0.08988235   0.5709343   0.9970440 0.4290759   0.78  990   20 6746  744
 [80,] 0.9080000   0.09200000   0.5599769   0.9971918 0.4400320   0.79  971   19 6747  763
 [81,] 0.9061176   0.09388235   0.5501730   0.9973396 0.4498349   0.80  954   18 6748  780
 [82,] 0.9047059   0.09529412   0.5426759   0.9974874 0.4573310   0.81  941   17 6749  793
 [83,] 0.9032941   0.09670588   0.5346021   0.9977830 0.4654032   0.82  927   15 6751  807
 [84,] 0.9014118   0.09858824   0.5247982   0.9979308 0.4752064   0.83  910   14 6752  824
 [85,] 0.9000000   0.10000000   0.5167243   0.9982264 0.4832789   0.84  896   12 6754  838
 [86,] 0.8980000   0.10200000   0.5063437   0.9983742 0.4936590   0.85  878   11 6755  856
 [87,] 0.8962353   0.10376471   0.4976932   0.9983742 0.5023094   0.86  863   11 6755  871
 [88,] 0.8951765   0.10482353   0.4907728   0.9988176 0.5092286   0.87  851    8 6758  883
 [89,] 0.8932941   0.10670588   0.4815456   0.9988176 0.5184558   0.88  835    8 6758  899
 [90,] 0.8920000   0.10800000   0.4728950   0.9994088 0.5271053   0.89  820    4 6762  914
 [91,] 0.8901176   0.10988235   0.4636678   0.9994088 0.5363325   0.90  804    4 6762  930
 [92,] 0.8882353   0.11176471   0.4538639   0.9995566 0.5461363   0.91  787    3 6763  947
 [93,] 0.8860000   0.11400000   0.4429066   0.9995566 0.5570936   0.92  768    3 6763  966
 [94,] 0.8838824   0.11611765   0.4313725   0.9998522 0.5686275   0.93  748    1 6765  986
 [95,] 0.8802353   0.11976471   0.4134948   0.9998522 0.5865052   0.94  717    1 6765 1017
 [96,] 0.8767059   0.12329412   0.3961938   0.9998522 0.6038062   0.95  687    1 6765 1047
 [97,] 0.8737647   0.12623529   0.3817762   0.9998522 0.6182238   0.96  662    1 6765 1072
 [98,] 0.8695294   0.13047059   0.3604383   1.0000000 0.6395617   0.97  625    0 6766 1109
 [99,] 0.8649412   0.13505882   0.3379469   1.0000000 0.6620531   0.98  586    0 6766 1148
[100,] 0.8583529   0.14164706   0.3056517   1.0000000 0.6943483   0.99  530    0 6766 1204
 [ reached 'max' / getOption("max.print") -- omitted 2 rows ]
> 
> #COMPUTING AREA UNDER THE ROC CURVE
> sensitivity<- sensitivity[order(sensitivity)]
> oneminusspec<- oneminusspec[order(oneminusspec)]
> 
> library(Hmisc) #Harrell Miscellaneous packages
> lagx<- Lag(oneminusspec,shift=1)
> lagy<- Lag(sensitivity, shift=1)
> lagx[is.na(lagx)]<- 0
> lagy[is.na(lagy)]<- 0
> trapezoid<- (oneminusspec-lagx)*(sensitivity+lagy)/2
> print(AUC<- sum(trapezoid))
[1] 0.7689956
> 
> 
> ####################################################################
> #FITTING ANN WITH LOGISTIC ACTIVATION FUNCTION AND C(2,3) LAYERS
> ann.log23.class<- neuralnet(diabetes ~ gender + age + hypertension + heart_disease + smoking_history + bmi + HbA1c_level + blood_glucose_level, 
+ data=train, hidden=c(2,3), act.fct="logistic", stepmax=1e7)
> 
> #PLOTTING THE DIAGRAM
> plot(ann.log23.class)
> 
> #COMPUTING PREDICTION ACCURACY FOR TESTING DATA
> pred.prob<- predict(ann.log23.class, test.x)[,1]
> 
> match<- c()
> pred.y<- c()
> for (i in 1:length(test.y)){
+   pred.y[i]<- ifelse(pred.prob[i]>0.5,1,0)
+   match[i]<- ifelse(test.y[i]==pred.y[i],1,0)
+ }
> 
> print(paste("accuracy=", round(mean(match), digits=4)))
[1] "accuracy= 0.9209"
> #calculating confusion matrix
> tp <- sum(pred.y == 1 & test.y == 1)
> fp <- sum(pred.y == 1 & test.y == 0)
> tn <- sum(pred.y == 0 & test.y == 0)
> fn <- sum(pred.y == 0 & test.y == 1)
> total <- length(test.y)
> 
> # Metrics
> accuracy <- (tp + tn) / total
> misclassrate <- (fp + fn) / total
> sensitivity <- tp / (tp + fn)
> FNR <- fn / (tp + fn)
> specificity <- tn / (fp + tn)
> FPR <- fp / (fp + tn)
> precision <- tp / (tp + fp)
> NPV <- tn / (fn + tn)
> F1score <- 2 * tp / (2 * tp + fn + fp)
> 
> # Print results
> print("ANN Confusion Matrix Results:")
[1] "ANN Confusion Matrix Results:"
> print(paste("TP =", tp, " FP =", fp, " TN =", tn, " FN =", fn, "Total =", total))
[1] "TP = 1210  FP = 148  TN = 6618  FN = 524 Total = 8500"
> print(paste("Accuracy =", round(accuracy, 4)))
[1] "Accuracy = 0.9209"
> print(paste("MisclassRate =", round(misclassrate, 4)))
[1] "MisclassRate = 0.0791"
> print(paste("Sensitivity =", round(sensitivity, 4)))
[1] "Sensitivity = 0.6978"
> print(paste("FNR =", round(FNR, 4)))
[1] "FNR = 0.3022"
> print(paste("Specificity =", round(specificity, 4)))
[1] "Specificity = 0.9781"
> print(paste("FPR =", round(FPR, 4)))
[1] "FPR = 0.0219"
> print(paste("Precision =", round(precision, 4)))
[1] "Precision = 0.891"
> print(paste("NPV =", round(NPV, 4)))
[1] "NPV = 0.9266"
> print(paste("F1 Score =", round(F1score, 4)))
[1] "F1 Score = 0.7827"
> ####ROC CURVE CODE
> 
> #COMPUTING CONFUSION MATRICES AND PERFORMANCE MEASURES FOR TESTING SET
> #FOR A RANGE OF CUT-OFFS
> 
> tpos<- matrix(NA, nrow=nrow(test), ncol=102)
> fpos<- matrix(NA, nrow=nrow(test), ncol=102)
> tneg<- matrix(NA, nrow=nrow(test), ncol=102)
> fneg<- matrix(NA, nrow=nrow(test), ncol=102)
> 
> 
> for (i in 0:101) {
+   tpos[,i+1]<- ifelse(test.y==1 & pred.prob>=0.01*i,1,0)
+   fpos[,i+1]<- ifelse(test.y==0 & pred.prob>=0.01*i, 1,0)
+   tneg[,i+1]<- ifelse(test.y==0 & pred.prob<0.01*i,1,0)
+   fneg[,i+1]<- ifelse(test.y==1 & pred.prob<0.01*i,1,0)
+ }
> 
> tp<- c()
> fp<- c()
> tn<- c()
> fn<- c()
> accuracy<- c()
> misclassrate<- c()
> sensitivity<- c()
> specificity<- c()
> oneminusspec<- c()
> cutoff<- c()
> 
> 
> for (i in 1:102) {
+   tp[i]<- sum(tpos[,i])
+   fp[i]<- sum(fpos[,i])
+   tn[i]<- sum(tneg[,i])
+   fn[i]<- sum(fneg[,i])
+   total<- nrow(test)
+   accuracy[i]<- (tp[i]+tn[i])/total
+   misclassrate[i]<- (fp[i]+fn[i])/total
+   sensitivity[i]<- tp[i]/(tp[i]+fn[i])
+   specificity[i]<- tn[i]/(fp[i]+tn[i])
+   oneminusspec[i]<- fp[i]/(fp[i]+tn[i])
+   cutoff[i]<- 0.01*(i-1)
+ }
> 
> #PLOTTING ROC CURVE
> plot(oneminusspec, sensitivity, type="l", lty=1, main="The Receiver 
+ Operating Characteristic Curve", xlab="1-Specificity", ylab="Sensitivity")
> points(oneminusspec, sensitivity, pch=0) #pch=plot character, 0=square
> 
> #REPORTING MEASURES FOR THE POINT ON ROC CURVE CLOSEST TO THE IDEAL POINT (0,1)
> distance<- c()
> for (i in 1:102)
+   distance[i]<- sqrt(oneminusspec[i]^2+(1-sensitivity[i])^2)
> 
> measures<- cbind(accuracy, misclassrate, sensitivity, specificity, distance, cutoff, tp, fp, tn, fn)
> min.dist<- min(distance)
> print(measures)
        accuracy misclassrate sensitivity specificity  distance cutoff   tp   fp   tn   fn
  [1,] 0.2075294   0.79247059 1.000000000 0.004433934 0.9955661   0.00 1734 6736   30    0
  [2,] 0.6168235   0.38317647 0.997693195 0.519213716 0.4807918   0.01 1730 3253 3513    4
  [3,] 0.6782353   0.32176471 0.991926182 0.597842152 0.4022389   0.02 1720 2721 4045   14
  [4,] 0.7110588   0.28894118 0.989042676 0.639816731 0.3603499   0.03 1715 2437 4329   19
  [5,] 0.7358824   0.26411765 0.985005767 0.672036654 0.3283059   0.04 1708 2219 4547   26
  [6,] 0.7551765   0.24482353 0.979238754 0.697753473 0.3029587   0.05 1698 2045 4721   36
  [7,] 0.7721176   0.22788235 0.974048443 0.720366539 0.2808351   0.06 1689 1892 4874   45
  [8,] 0.7867059   0.21329412 0.967704729 0.740319243 0.2616813   0.07 1678 1757 5009   56
  [9,] 0.7992941   0.20070588 0.963091119 0.757315992 0.2454746   0.08 1670 1642 5124   64
 [10,] 0.8075294   0.19247059 0.955594002 0.769583210 0.2346567   0.09 1657 1559 5207   77
 [11,] 0.8152941   0.18470588 0.950403691 0.780668046 0.2248695   0.10 1648 1484 5282   86
 [12,] 0.8234118   0.17658824 0.946366782 0.791900680 0.2148996   0.11 1641 1408 5358   93
 [13,] 0.8309412   0.16905882 0.938869666 0.803281111 0.2059982   0.12 1628 1331 5435  106
 [14,] 0.8370588   0.16294118 0.933102653 0.812444576 0.1991288   0.13 1618 1269 5497  116
 [15,] 0.8441176   0.15588235 0.926758939 0.822938221 0.1916119   0.14 1607 1198 5568  127
 [16,] 0.8507059   0.14929412 0.922145329 0.832397281 0.1848027   0.15 1599 1134 5632  135
 [17,] 0.8552941   0.14470588 0.918685121 0.839048182 0.1803264   0.16 1593 1089 5677  141
 [18,] 0.8615294   0.13847059 0.914071511 0.848063849 0.1745517   0.17 1585 1028 5738  149
 [19,] 0.8668235   0.13317647 0.910034602 0.855749335 0.1700060   0.18 1578  976 5790  156
 [20,] 0.8711765   0.12882353 0.903690888 0.862843630 0.1675927   0.19 1567  928 5838  167
 [21,] 0.8752941   0.12470588 0.895617070 0.870085723 0.1666539   0.20 1553  879 5887  181
 [22,] 0.8788235   0.12117647 0.884659746 0.877327816 0.1683800   0.21 1534  830 5936  200
 [23,] 0.8824706   0.11752941 0.871972318 0.885161100 0.1719856   0.22 1512  777 5989  222
 [24,] 0.8864706   0.11352941 0.863321799 0.892403192 0.1739483   0.23 1497  728 6038  237
 [25,] 0.8895294   0.11047059 0.855247982 0.898315105 0.1768982   0.24 1483  688 6078  251
 [26,] 0.8929412   0.10705882 0.846020761 0.904966007 0.1809449   0.25 1467  643 6123  267
 [27,] 0.8974118   0.10258824 0.837370242 0.912799291 0.1845329   0.26 1452  590 6176  282
 [28,] 0.9008235   0.09917647 0.829296424 0.919154597 0.1888801   0.27 1438  547 6219  296
 [29,] 0.9043529   0.09564706 0.822376009 0.925362105 0.1926684   0.28 1426  505 6261  308
 [30,] 0.9080000   0.09200000 0.816608997 0.931421815 0.1957938   0.29 1416  464 6302  318
 [31,] 0.9103529   0.08964706 0.806228374 0.937038132 0.2037441   0.30 1398  426 6340  336
 [32,] 0.9130588   0.08694118 0.801614764 0.941619864 0.2067969   0.31 1390  395 6371  344
 [33,] 0.9147059   0.08529412 0.795271050 0.945314809 0.2119066   0.32 1379  370 6396  355
 [34,] 0.9164706   0.08352941 0.787773933 0.949453148 0.2181625   0.33 1366  342 6424  368
 [35,] 0.9172941   0.08270588 0.783160323 0.951670115 0.2221604   0.34 1358  327 6439  376
 [36,] 0.9183529   0.08164706 0.777970012 0.954330476 0.2266782   0.35 1349  309 6457  385
 [37,] 0.9182353   0.08176471 0.769319493 0.956399645 0.2347647   0.36 1334  295 6471  400
 [38,] 0.9187059   0.08129412 0.762399077 0.958764410 0.2411526   0.37 1322  279 6487  412
 [39,] 0.9190588   0.08094118 0.757208766 0.960537984 0.2459773   0.38 1313  267 6499  421
 [40,] 0.9192941   0.08070588 0.751441753 0.962311558 0.2513993   0.39 1303  255 6511  431
 [41,] 0.9200000   0.08000000 0.746251442 0.964528525 0.2562158   0.40 1294  240 6526  440
 [42,] 0.9197647   0.08023529 0.739907728 0.965858705 0.2623235   0.41 1283  231 6535  451
 [43,] 0.9197647   0.08023529 0.735870819 0.966893290 0.2661959   0.42 1276  224 6542  458
 [44,] 0.9197647   0.08023529 0.729527105 0.968519066 0.2722988   0.43 1265  213 6553  469
 [45,] 0.9197647   0.08023529 0.724336794 0.969849246 0.2773072   0.44 1256  204 6562  478
 [46,] 0.9200000   0.08000000 0.718569781 0.971622820 0.2828573   0.45 1246  192 6574  488
 [47,] 0.9200000   0.08000000 0.713956171 0.972805202 0.2873337   0.46 1238  184 6582  496
 [48,] 0.9208235   0.07917647 0.711072664 0.974578776 0.2900435   0.47 1233  172 6594  501
 [49,] 0.9209412   0.07905882 0.707035755 0.975761159 0.2939653   0.48 1226  164 6602  508
 [50,] 0.9215294   0.07847059 0.704152249 0.977239137 0.2967220   0.49 1221  154 6612  513
 [51,] 0.9209412   0.07905882 0.697808535 0.978125924 0.3029821   0.50 1210  148 6618  524
 [52,] 0.9217647   0.07823529 0.694348328 0.980047295 0.3063022   0.51 1204  135 6631  530
 [53,] 0.9215294   0.07847059 0.689734717 0.980934082 0.3108505   0.52 1196  129 6637  538
 [54,] 0.9212941   0.07870588 0.686851211 0.981377476 0.3137020   0.53 1191  126 6640  543
 [55,] 0.9205882   0.07941176 0.679354095 0.982412060 0.3211279   0.54 1178  119 6647  556
 [56,] 0.9201176   0.07988235 0.674740484 0.983003252 0.3257033   0.55 1170  115 6651  564
 [57,] 0.9195294   0.08047059 0.668396770 0.983890038 0.3319943   0.56 1159  109 6657  575
 [58,] 0.9200000   0.08000000 0.664936563 0.985368017 0.3353828   0.57 1153   99 6667  581
 [59,] 0.9198824   0.08011765 0.661476355 0.986107006 0.3388086   0.58 1147   94 6672  587
 [60,] 0.9192941   0.08070588 0.655709343 0.986845995 0.3445418   0.59 1137   89 6677  597
 [61,] 0.9195294   0.08047059 0.652825836 0.987880579 0.3473856   0.60 1132   82 6684  602
 [62,] 0.9191765   0.08082353 0.648788927 0.988471771 0.3514002   0.61 1125   78 6688  609
 [63,] 0.9192941   0.08070588 0.647058824 0.989062962 0.3531106   0.62 1122   74 6692  612
 [64,] 0.9194118   0.08058824 0.644752018 0.989801951 0.3553943   0.63 1118   69 6697  616
 [65,] 0.9187059   0.08129412 0.637831603 0.990688738 0.3622881   0.64 1106   63 6703  628
 [66,] 0.9183529   0.08164706 0.633217993 0.991427727 0.3668822   0.65 1098   58 6708  636
 [67,] 0.9178824   0.08211765 0.629757785 0.991723322 0.3703347   0.66 1092   56 6710  642
 [68,] 0.9176471   0.08235294 0.625144175 0.992610109 0.3749287   0.67 1084   50 6716  650
 [69,] 0.9169412   0.08305882 0.619377163 0.993201301 0.3806836   0.68 1074   46 6720  660
 [70,] 0.9165882   0.08341176 0.617070358 0.993349098 0.3829874   0.69 1070   45 6721  664
 [71,] 0.9161176   0.08388235 0.613033449 0.993792492 0.3870163   0.70 1063   42 6724  671
 [72,] 0.9158824   0.08411765 0.610149942 0.994235885 0.3898927   0.71 1058   39 6727  676
 [73,] 0.9144706   0.08552941 0.603229527 0.994235885 0.3968123   0.72 1046   39 6727  688
 [74,] 0.9134118   0.08658824 0.597462514 0.994383683 0.4025767   0.73 1036   38 6728  698
 [75,] 0.9129412   0.08705882 0.594579008 0.994531481 0.4054579   0.74 1031   37 6729  703
 [76,] 0.9125882   0.08741176 0.591695502 0.994827077 0.4083373   0.75 1026   35 6731  708
 [77,] 0.9118824   0.08811765 0.587081892 0.995122672 0.4129469   0.76 1018   33 6733  716
 [78,] 0.9117647   0.08823529 0.585351788 0.995418268 0.4146735   0.77 1015   31 6735  719
 [79,] 0.9112941   0.08870588 0.581314879 0.995861661 0.4187056   0.78 1008   28 6738  726
 [80,] 0.9112941   0.08870588 0.578431373 0.996600650 0.4215823   0.79 1003   23 6743  731
 [81,] 0.9105882   0.08941176 0.573817762 0.996896246 0.4261935   0.80  995   21 6745  739
 [82,] 0.9100000   0.09000000 0.570357555 0.997044044 0.4296526   0.81  989   20 6746  745
 [83,] 0.9094118   0.09058824 0.566320646 0.997339639 0.4336875   0.82  982   18 6748  752
 [84,] 0.9083529   0.09164706 0.560553633 0.997487437 0.4394535   0.83  972   17 6749  762
 [85,] 0.9069412   0.09305882 0.552479815 0.997783033 0.4475257   0.84  958   15 6751  776
 [86,] 0.9050588   0.09494118 0.543252595 0.997783033 0.4567528   0.85  942   15 6751  792
 [87,] 0.9038824   0.09611765 0.537485582 0.997783033 0.4625197   0.86  932   15 6751  802
 [88,] 0.9029412   0.09705882 0.532295271 0.997930831 0.4677093   0.87  923   14 6752  811
 [89,] 0.9014118   0.09858824 0.524221453 0.998078628 0.4757824   0.88  909   13 6753  825
 [90,] 0.8997647   0.10023529 0.515570934 0.998226426 0.4844323   0.89  894   12 6754  840
 [91,] 0.8974118   0.10258824 0.502883506 0.998522022 0.4971187   0.90  872   10 6756  862
 [92,] 0.8961176   0.10388235 0.495386390 0.998817617 0.5046150   0.91  859    8 6758  875
 [93,] 0.8941176   0.10588235 0.485005767 0.998965415 0.5149953   0.92  841    7 6759  893
 [94,] 0.8922353   0.10776471 0.475778547 0.998965415 0.5242225   0.93  825    7 6759  909
 [95,] 0.8897647   0.11023529 0.462514418 0.999261011 0.5374861   0.94  802    5 6761  932
 [96,] 0.8872941   0.11270588 0.449826990 0.999408809 0.5501733   0.95  780    4 6762  954
 [97,] 0.8850588   0.11494118 0.437716263 0.999704404 0.5622838   0.96  759    2 6764  975
 [98,] 0.8820000   0.11800000 0.422145329 0.999852202 0.5778547   0.97  732    1 6765 1002
 [99,] 0.8750588   0.12494118 0.388119954 0.999852202 0.6118801   0.98  673    1 6765 1061
[100,] 0.8671765   0.13282353 0.349480969 0.999852202 0.6505190   0.99  606    1 6765 1128
 [ reached 'max' / getOption("max.print") -- omitted 2 rows ]
> 
> #COMPUTING AREA UNDER THE ROC CURVE
> sensitivity<- sensitivity[order(sensitivity)]
> oneminusspec<- oneminusspec[order(oneminusspec)]
> 
> library(Hmisc) #Harrell Miscellaneous packages
> lagx<- Lag(oneminusspec,shift=1)
> lagy<- Lag(sensitivity, shift=1)
> lagx[is.na(lagx)]<- 0
> lagy[is.na(lagy)]<- 0
> trapezoid<- (oneminusspec-lagx)*(sensitivity+lagy)/2
> print(AUC<- sum(trapezoid))
[1] 0.9559492
> 
> 
> 
> ####################################################################
> # Scale the input features (excluding the target variable)
> scaled_train_x <- as.data.frame(scale(train[, -9]))  # exclude 'diabetes' column
> scaled_test_x <- as.data.frame(scale(test[, -9]))
> 
> # Add back the diabetes column
> scaled_train <- cbind(scaled_train_x, diabetes = train$diabetes)
> scaled_test <- cbind(scaled_test_x, diabetes = test$diabetes)
> 
> # Update data matrices for prediction
> train.x <- data.matrix(scaled_train[, -9])
> train.y <- data.matrix(scaled_train[, 9])
> test.x <- data.matrix(scaled_test[, -9])
> test.y <- data.matrix(scaled_test[, 9])
> 
> #FITTING ANN WITH TANH ACTIVATION FUNCTION
> ann.tanh.class<- neuralnet(diabetes ~ gender + age + hypertension + heart_disease + smoking_history + bmi + HbA1c_level + blood_glucose_level, 
+ data=train, hidden=2, act.fct="tanh", stepmax=1e7)
> 
> #PLOTTING THE DIAGRAM
> plot(ann.tanh.class)
> 
> #COMPUTING PREDICTION ACCURACY FOR TESTING DATA
> pred.prob<- predict(ann.tanh.class, test.x)[,1]
> 
> match<- c()
> pred.y<- c()
> for (i in 1:length(test.y)){
+   pred.y[i]<- ifelse(pred.prob[i]>0.5,1,0)
+   match[i]<- ifelse(test.y[i]==pred.y[i],1,0)
+ }
> 
> print(paste("accuracy=", round(mean(match), digits=4)))
[1] "accuracy= 0.6933"
> 
> 
> #calculating confusion matrix
> tp <- sum(pred.y == 1 & test.y == 1)
> fp <- sum(pred.y == 1 & test.y == 0)
> tn <- sum(pred.y == 0 & test.y == 0)
> fn <- sum(pred.y == 0 & test.y == 1)
> total <- length(test.y)
> 
> # Metrics
> accuracy <- (tp + tn) / total
> misclassrate <- (fp + fn) / total
> sensitivity <- tp / (tp + fn)
> FNR <- fn / (tp + fn)
> specificity <- tn / (fp + tn)
> FPR <- fp / (fp + tn)
> precision <- tp / (tp + fp)
> NPV <- tn / (fn + tn)
> F1score <- 2 * tp / (2 * tp + fn + fp)
> 
> # Print results
> print("ANN Confusion Matrix Results:")
[1] "ANN Confusion Matrix Results:"
> print(paste("TP =", tp, " FP =", fp, " TN =", tn, " FN =", fn, "Total =", total))
[1] "TP = 411  FP = 1284  TN = 5482  FN = 1323 Total = 8500"
> print(paste("Accuracy =", round(accuracy, 4)))
[1] "Accuracy = 0.6933"
> print(paste("MisclassRate =", round(misclassrate, 4)))
[1] "MisclassRate = 0.3067"
> print(paste("Sensitivity =", round(sensitivity, 4)))
[1] "Sensitivity = 0.237"
> print(paste("FNR =", round(FNR, 4)))
[1] "FNR = 0.763"
> print(paste("Specificity =", round(specificity, 4)))
[1] "Specificity = 0.8102"
> print(paste("FPR =", round(FPR, 4)))
[1] "FPR = 0.1898"
> print(paste("Precision =", round(precision, 4)))
[1] "Precision = 0.2425"
> print(paste("NPV =", round(NPV, 4)))
[1] "NPV = 0.8056"
> print(paste("F1 Score =", round(F1score, 4)))
[1] "F1 Score = 0.2397"
> 
> ####ROC CURVE CODE
> 
> #COMPUTING CONFUSION MATRICES AND PERFORMANCE MEASURES FOR TESTING SET
> #FOR A RANGE OF CUT-OFFS
> pred.prob <- (pred.prob + 1) / 2
> 
> tpos<- matrix(NA, nrow=nrow(test), ncol=102)
> fpos<- matrix(NA, nrow=nrow(test), ncol=102)
> tneg<- matrix(NA, nrow=nrow(test), ncol=102)
> fneg<- matrix(NA, nrow=nrow(test), ncol=102)
> 
> 
> for (i in 0:101) {
+   tpos[,i+1]<- ifelse(test.y==1 & pred.prob>=0.01*i,1,0)
+   fpos[,i+1]<- ifelse(test.y==0 & pred.prob>=0.01*i, 1,0)
+   tneg[,i+1]<- ifelse(test.y==0 & pred.prob<0.01*i,1,0)
+   fneg[,i+1]<- ifelse(test.y==1 & pred.prob<0.01*i,1,0)
+ }
> 
> tp<- c()
> fp<- c()
> tn<- c()
> fn<- c()
> accuracy<- c()
> misclassrate<- c()
> sensitivity<- c()
> specificity<- c()
> oneminusspec<- c()
> cutoff<- c()
> 
> 
> for (i in 1:102) {
+   tp[i]<- sum(tpos[,i])
+   fp[i]<- sum(fpos[,i])
+   tn[i]<- sum(tneg[,i])
+   fn[i]<- sum(fneg[,i])
+   total<- nrow(test)
+   accuracy[i]<- (tp[i]+tn[i])/total
+   misclassrate[i]<- (fp[i]+fn[i])/total
+   sensitivity[i]<- tp[i]/(tp[i]+fn[i])
+   specificity[i]<- tn[i]/(fp[i]+tn[i])
+   oneminusspec[i]<- fp[i]/(fp[i]+tn[i])
+   cutoff[i]<- 0.01*(i-1)
+ }
> 
> #PLOTTING ROC CURVE
> plot(oneminusspec, sensitivity, type="l", lty=1, main="The Receiver 
+ Operating Characteristic Curve", xlab="1-Specificity", ylab="Sensitivity")
> points(oneminusspec, sensitivity, pch=0) #pch=plot character, 0=square
> 
> #REPORTING MEASURES FOR THE POINT ON ROC CURVE CLOSEST TO THE IDEAL POINT (0,1)
> distance<- c()
> for (i in 1:102)
+   distance[i]<- sqrt(oneminusspec[i]^2+(1-sensitivity[i])^2)
> 
> measures<- cbind(accuracy, misclassrate, sensitivity, specificity, distance, cutoff, tp, fp, tn, fn)
> min.dist<- min(distance)
> print(measures)
        accuracy misclassrate sensitivity specificity  distance cutoff   tp   fp   tn   fn
  [1,] 0.4516471    0.5483529   0.9400231   0.3264854 0.6761799   0.00 1630 4557 2209  104
  [2,] 0.4611765    0.5388235   0.9354095   0.3396394 0.6635119   0.01 1622 4468 2298  112
  [3,] 0.4717647    0.5282353   0.9342561   0.3532368 0.6500961   0.02 1620 4376 2390  114
  [4,] 0.4795294    0.5204706   0.9261822   0.3650606 0.6392160   0.03 1606 4296 2470  128
  [5,] 0.4865882    0.5134118   0.9227220   0.3748153 0.6299427   0.04 1600 4230 2536  134
  [6,] 0.4945882    0.5054118   0.9181084   0.3860479 0.6193896   0.05 1592 4154 2612  142
  [7,] 0.5004706    0.4995294   0.9152249   0.3941768 0.6117259   0.06 1587 4099 2667  147
  [8,] 0.5047059    0.4952941   0.9088812   0.4011233 0.6057689   0.07 1576 4052 2714  158
  [9,] 0.5103529    0.4896471   0.9048443   0.4092521 0.5983625   0.08 1569 3997 2769  165
 [10,] 0.5162353    0.4837647   0.9025375   0.4172332 0.5908604   0.09 1565 3943 2823  169
 [11,] 0.5203529    0.4796471   0.8990773   0.4232929 0.5854711   0.10 1559 3902 2864  175
 [12,] 0.5244706    0.4755294   0.8944637   0.4296482 0.5800337   0.11 1551 3859 2907  183
 [13,] 0.5297647    0.4702353   0.8910035   0.4371859 0.5732712   0.12 1545 3808 2958  189
 [14,] 0.5341176    0.4658824   0.8840830   0.4444280 0.5675359   0.13 1533 3759 3007  201
 [15,] 0.5396471    0.4603529   0.8823529   0.4518179 0.5606643   0.14 1530 3709 3057  204
 [16,] 0.5440000    0.4560000   0.8794694   0.4580254 0.5552153   0.15 1525 3667 3099  209
 [17,] 0.5475294    0.4524706   0.8760092   0.4633461 0.5507913   0.16 1519 3631 3135  215
 [18,] 0.5525882    0.4474118   0.8731257   0.4704404 0.5445461   0.17 1514 3583 3183  220
 [19,] 0.5569412    0.4430588   0.8708189   0.4765001 0.5392030   0.18 1510 3542 3224  224
 [20,] 0.5595294    0.4404706   0.8656286   0.4810819 0.5360333   0.19 1501 3511 3255  233
 [21,] 0.5620000    0.4380000   0.8627451   0.4849246 0.5330493   0.20 1496 3485 3281  238
 [22,] 0.5663529    0.4336471   0.8598616   0.4911321 0.5278118   0.21 1491 3443 3323  243
 [23,] 0.5709412    0.4290588   0.8581315   0.4973396 0.5222970   0.22 1488 3401 3365  246
 [24,] 0.5748235    0.4251765   0.8552480   0.5029560 0.5176929   0.23 1483 3363 3403  251
 [25,] 0.5778824    0.4221176   0.8517878   0.5076855 0.5141405   0.24 1477 3331 3435  257
 [26,] 0.5812941    0.4187059   0.8500577   0.5124150 0.5101194   0.25 1474 3299 3467  260
 [27,] 0.5849412    0.4150588   0.8465975   0.5178835 0.5059334   0.26 1468 3262 3504  266
 [28,] 0.5858824    0.4141176   0.8414072   0.5203961 0.5051451   0.27 1459 3245 3521  275
 [29,] 0.5888235    0.4111765   0.8367935   0.5252734 0.5019977   0.28 1451 3212 3554  283
 [30,] 0.5916471    0.4083529   0.8316032   0.5301508 0.4991150   0.29 1442 3179 3587  292
 [31,] 0.5943529    0.4056471   0.8264129   0.5348803 0.4964563   0.30 1433 3147 3619  301
 [32,] 0.5980000    0.4020000   0.8229527   0.5403488 0.4925698   0.31 1427 3110 3656  307
 [33,] 0.6014118    0.3985882   0.8194925   0.5455217 0.4890127   0.32 1421 3075 3691  313
 [34,] 0.6055294    0.3944706   0.8171857   0.5512858 0.4845260   0.33 1417 3036 3730  317
 [35,] 0.6101176    0.3898824   0.8160323   0.5573456 0.4793611   0.34 1415 2995 3771  319
 [36,] 0.6115294    0.3884706   0.8085352   0.5610405 0.4788990   0.35 1402 2970 3796  332
 [37,] 0.6152941    0.3847059   0.8039216   0.5669524 0.4753703   0.36 1394 2930 3836  340
 [38,] 0.6181176    0.3818824   0.7998847   0.5715341 0.4728944   0.37 1387 2899 3867  347
 [39,] 0.6221176    0.3778824   0.7993080   0.5767071 0.4684594   0.38 1386 2864 3902  348
 [40,] 0.6268235    0.3731765   0.7958478   0.5835058 0.4638379   0.39 1380 2818 3948  354
 [41,] 0.6315294    0.3684706   0.7941176   0.5898611 0.4589134   0.40 1377 2775 3991  357
 [42,] 0.6350588    0.3649412   0.7900807   0.5953296 0.4558774   0.41 1370 2738 4028  364
 [43,] 0.6389412    0.3610588   0.7866205   0.6010937 0.4523904   0.42 1364 2699 4067  370
 [44,] 0.6428235    0.3571765   0.7831603   0.6068578 0.4489769   0.43 1358 2660 4106  376
 [45,] 0.6462353    0.3537647   0.7791234   0.6121785 0.4463093   0.44 1351 2624 4142  383
 [46,] 0.6496471    0.3503529   0.7716263   0.6183860 0.4447289   0.45 1338 2582 4184  396
 [47,] 0.6556471    0.3443529   0.7647059   0.6276973 0.4404232   0.46 1326 2519 4247  408
 [48,] 0.6621176    0.3378824   0.7589389   0.6373042 0.4354983   0.47 1316 2454 4312  418
 [49,] 0.6681176    0.3318824   0.7497116   0.6472066 0.4325592   0.48 1300 2387 4379  434
 [50,] 0.6776471    0.3223529   0.7427912   0.6609518 0.4255702   0.49 1288 2294 4472  446
 [51,] 0.6828235    0.3171765   0.7364475   0.6690807 0.4230455   0.50 1277 2239 4527  457
 [52,] 0.6865882    0.3134118   0.7260669   0.6764706 0.4239229   0.51 1259 2189 4577  475
 [53,] 0.6916471    0.3083529   0.7191465   0.6845995 0.4223224   0.52 1247 2134 4632  487
 [54,] 0.6967059    0.3032941   0.7151096   0.6919894 0.4195630   0.53 1240 2084 4682  494
 [55,] 0.7004706    0.2995294   0.7006920   0.7004138 0.4234821   0.54 1215 2027 4739  519
 [56,] 0.7045882    0.2954118   0.6931949   0.7075081 0.4238878   0.55 1202 1979 4787  532
 [57,] 0.7078824    0.2921176   0.6793541   0.7151936 0.4288688   0.56 1178 1927 4839  556
 [58,] 0.7092941    0.2907059   0.6597463   0.7219923 0.4393869   0.57 1144 1881 4885  590
 [59,] 0.7082353    0.2917647   0.6366782   0.7265740 0.4547136   0.58 1104 1850 4916  630
 [60,] 0.7092941    0.2907059   0.6089965   0.7349985 0.4723447   0.59 1056 1793 4973  678
 [61,] 0.6801176    0.3198824   0.4279123   0.7447532 0.6264465   0.60  742 1727 5039  992
 [62,] 0.6732941    0.3267059   0.3564014   0.7545078 0.6888291   0.61  618 1661 5105 1116
 [63,] 0.6732941    0.3267059   0.3339100   0.7602719 0.7079162   0.62  579 1622 5144 1155
 [64,] 0.6732941    0.3267059   0.3148789   0.7651493 0.7242553   0.63  546 1589 5177 1188
 [65,] 0.6755294    0.3244706   0.3044983   0.7706178 0.7323516   0.64  528 1552 5214 1206
 [66,] 0.6772941    0.3227059   0.2941176   0.7754951 0.7407242   0.65  510 1519 5247 1224
 [67,] 0.6787059    0.3212941   0.2866205   0.7791901 0.7467712   0.66  497 1494 5272 1237
 [68,] 0.6791765    0.3208235   0.2773933   0.7821460 0.7547323   0.67  481 1474 5292 1253
 [69,] 0.6805882    0.3194118   0.2710496   0.7855454 0.7598417   0.68  470 1451 5315 1264
 [70,] 0.6821176    0.3178824   0.2647059   0.7890925 0.7649441   0.69  459 1427 5339 1275
 [71,] 0.6845882    0.3154118   0.2595156   0.7935265 0.7687318   0.70  450 1397 5369 1284
 [72,] 0.6875294    0.3124706   0.2554787   0.7982560 0.7713706   0.71  443 1365 5401 1291
 [73,] 0.6894118    0.3105882   0.2508651   0.8018031 0.7749098   0.72  435 1341 5425 1299
 [74,] 0.6912941    0.3087059   0.2474048   0.8050547 0.7774337   0.73  429 1319 5447 1305
 [75,] 0.6928235    0.3071765   0.2427912   0.8081584 0.7811327   0.74  421 1298 5468 1313
 [76,] 0.6932941    0.3067059   0.2370242   0.8102276 0.7862224   0.75  411 1284 5482 1323
 [77,] 0.6951765    0.3048235   0.2329873   0.8136270 0.7893310   0.76  404 1261 5505 1330
 [78,] 0.6968235    0.3031765   0.2301038   0.8164351 0.7914772   0.77  399 1242 5524 1335
 [79,] 0.6992941    0.3007059   0.2295271   0.8196867 0.7912910   0.78  398 1220 5546 1336
 [80,] 0.7010588    0.2989412   0.2260669   0.8227904 0.7939620   0.79  392 1199 5567 1342
 [81,] 0.7025882    0.2974118   0.2237601   0.8253030 0.7956554   0.80  388 1182 5584 1346
 [82,] 0.7051765    0.2948235   0.2226067   0.8288501 0.7960104   0.81  386 1158 5608 1348
 [83,] 0.7064706    0.2935294   0.2202999   0.8310671 0.7977911   0.82  382 1143 5623 1352
 [84,] 0.7070588    0.2929412   0.2179931   0.8323973 0.7997659   0.83  378 1134 5632 1356
 [85,] 0.7084706    0.2915294   0.2151096   0.8349098 0.8020647   0.84  373 1117 5649 1361
 [86,] 0.7105882    0.2894118   0.2139562   0.8378658 0.8025911   0.85  371 1097 5669 1363
 [87,] 0.7117647    0.2882353   0.2116494   0.8399350 0.8044362   0.86  367 1083 5683 1367
 [88,] 0.7131765    0.2868235   0.2087659   0.8424475 0.8067678   0.87  362 1066 5700 1372
 [89,] 0.7151765    0.2848235   0.2064591   0.8455513 0.8084316   0.88  358 1045 5721 1376
 [90,] 0.7158824    0.2841176   0.2029988   0.8473249 0.8114928   0.89  352 1033 5733 1382
 [91,] 0.7180000    0.2820000   0.1983852   0.8511676 0.8153142   0.90  344 1007 5759 1390
 [92,] 0.7185882    0.2814118   0.1937716   0.8530890 0.8195041   0.91  336  994 5772 1398
 [93,] 0.7208235    0.2791765   0.1897347   0.8569317 0.8227991   0.92  329  968 5798 1405
 [94,] 0.7222353    0.2777647   0.1851211   0.8598877 0.8268368   0.93  321  948 5818 1413
 [95,] 0.7242353    0.2757647   0.1828143   0.8629914 0.8285915   0.94  317  927 5839 1417
 [96,] 0.7270588    0.2729412   0.1805075   0.8671298 0.8301942   0.95  313  899 5867 1421
 [97,] 0.7290588    0.2709412   0.1776240   0.8703813 0.8325283   0.96  308  877 5889 1426
 [98,] 0.7311765    0.2688235   0.1747405   0.8737807 0.8348560   0.97  303  854 5912 1431
 [99,] 0.7331765    0.2668235   0.1730104   0.8767366 0.8361254   0.98  300  834 5932 1434
[100,] 0.7355294    0.2644706   0.1707036   0.8802838 0.8378929   0.99  296  810 5956 1438
 [ reached 'max' / getOption("max.print") -- omitted 2 rows ]
> 
> #COMPUTING AREA UNDER THE ROC CURVE
> sensitivity<- sensitivity[order(sensitivity)]
> oneminusspec<- oneminusspec[order(oneminusspec)]
> 
> library(Hmisc) #Harrell Miscellaneous packages
> lagx<- Lag(oneminusspec,shift=1)
> lagy<- Lag(sensitivity, shift=1)
> lagx[is.na(lagx)]<- 0
> lagy[is.na(lagy)]<- 0
> trapezoid<- (oneminusspec-lagx)*(sensitivity+lagy)/2
> print(AUC<- sum(trapezoid))
[1] 0.3863296