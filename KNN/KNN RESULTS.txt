KNN RESULTS

> diabetes.data<- read.csv(file="C:/Users/sathv/OneDrive/Stats Research Project/diabetes_prediction_dataset.csv", 
+ header=TRUE, sep=",")
> 
> #SPLITTING DATA INTO 80% TRAINING AND 20% TESTING SETS 
> library(caTools)
> set.seed(704467) 
> sample<- sample.split(diabetes.data$diabetes, SplitRatio=0.80)
> train<- subset(diabetes.data, sample==TRUE)
> test<-  subset(diabetes.data, sample==FALSE)
> 
> train.x<- data.matrix(train[-9])
> train.y<- data.matrix(train[9])
> test.x<- data.matrix(test[-9])
> test.y<- data.matrix(test[9])
> 
> #TRAINING K-NEAREST NEIGHBOR BINARY CLASSIFIER
> library(caret)#classification and regression training
Loading required package: ggplot2

Attaching package: ‘ggplot2’

The following object is masked from ‘package:randomForest’:

    margin

Loading required package: lattice
> print(train(as.factor(diabetes)~., data=train, method="knn"))
k-Nearest Neighbors 

80000 samples
    8 predictor
    2 classes: '0', '1' 

No pre-processing
Resampling: Bootstrapped (25 reps) 
Summary of sample sizes: 80000, 80000, 80000, 80000, 80000, 80000, ... 
Resampling results across tuning parameters:

  k  Accuracy   Kappa    
  5  0.9462858  0.6072092
  7  0.9492072  0.6142885
  9  0.9503485  0.6133245

Accuracy was used to select the optimal model using the largest value.
The final value used for the model was k = 9.
> 
> #FITTING OPTIMAL KNN BINARY CLASSIFIER (k=9)
> knn.class<- knnreg(train.x, train.y, k=9)
> 
> #COMPUTING PREDICTION ACCURACY FOR TESTING DATA 
> pred.prob<- predict(knn.class, test.x)
> 
> len<- length(pred.prob)
> pred.y<- c()
> match<- c()
> for (i in 1:len){
+   pred.y[i]<- ifelse(pred.prob[i]>=0.5, 1,0)
+   match[i]<- ifelse(test.y[i]==pred.y[i], 1,0)
+ }
> print(paste("accuracy=",round(mean(match),digits=4)))
[1] "accuracy= 0.9558"
> 
> #alternative (frugal) way
> pred.y1<- floor(0.5+predict(knn.class, test.x))
> print(paste("accuracy=", round(1-mean(test.y!=pred.y1),digits=4)))
[1] "accuracy= 0.9558"
> 
> #calculating confusion matrix
> tp <- sum(pred.y == 1 & test.y == 1)
> fp <- sum(pred.y == 1 & test.y == 0)
> tn <- sum(pred.y == 0 & test.y == 0)
> fn <- sum(pred.y == 0 & test.y == 1)
> total <- length(test.y)
> 
> # Metrics
> accuracy <- (tp + tn) / total
> misclassrate <- (fp + fn) / total
> sensitivity <- tp / (tp + fn)
> FNR <- fn / (tp + fn)
> specificity <- tn / (fp + tn)
> FPR <- fp / (fp + tn)
> precision <- tp / (tp + fp)
> NPV <- tn / (fn + tn)
> F1score <- 2 * tp / (2 * tp + fn + fp)
> 
> # Print results
> print("KNN Confusion Matrix Results:")
[1] "KNN Confusion Matrix Results:"
> print(paste("TP =", tp, " FP =", fp, " TN =", tn, " FN =", fn, "Total =", total))
[1] "TP = 878  FP = 62  TN = 18238  FN = 822 Total = 20000"
> print(paste("Accuracy =", round(accuracy, 4)))
[1] "Accuracy = 0.9558"
> print(paste("MisclassRate =", round(misclassrate, 4)))
[1] "MisclassRate = 0.0442"
> print(paste("Sensitivity =", round(sensitivity, 4)))
[1] "Sensitivity = 0.5165"
> print(paste("FNR =", round(FNR, 4)))
[1] "FNR = 0.4835"
> print(paste("Specificity =", round(specificity, 4)))
[1] "Specificity = 0.9966"
> print(paste("FPR =", round(FPR, 4)))
[1] "FPR = 0.0034"
> print(paste("Precision =", round(precision, 4)))
[1] "Precision = 0.934"
> print(paste("NPV =", round(NPV, 4)))
[1] "NPV = 0.9569"
> print(paste("F1 Score =", round(F1score, 4)))
[1] "F1 Score = 0.6652"
> 
> ####ROC CURVE CODE
> 
> #COMPUTING CONFUSION MATRICES AND PERFORMANCE MEASURES FOR TESTING SET
> #FOR A RANGE OF CUT-OFFS
> 
> tpos<- matrix(NA, nrow=nrow(test), ncol=102)
> fpos<- matrix(NA, nrow=nrow(test), ncol=102)
> tneg<- matrix(NA, nrow=nrow(test), ncol=102)
> fneg<- matrix(NA, nrow=nrow(test), ncol=102)
> 
> 
> for (i in 0:101) {
+   tpos[,i+1]<- ifelse(test$diabetes==1 & pred.prob>=0.01*i,1,0)
+   fpos[,i+1]<- ifelse(test$diabetes==0 & pred.prob>=0.01*i, 1,0)
+   tneg[,i+1]<- ifelse(test$diabetes==0 & pred.prob<0.01*i,1,0)
+   fneg[,i+1]<- ifelse(test$diabetes==1 & pred.prob<0.01*i,1,0)
+ }
> 
> tp<- c()
> fp<- c()
> tn<- c()
> fn<- c()
> accuracy<- c()
> misclassrate<- c()
> sensitivity<- c()
> specificity<- c()
> oneminusspec<- c()
> cutoff<- c()
> 
> 
> for (i in 1:102) {
+   tp[i]<- sum(tpos[,i])
+   fp[i]<- sum(fpos[,i])
+   tn[i]<- sum(tneg[,i])
+   fn[i]<- sum(fneg[,i])
+   total<- nrow(test)
+   accuracy[i]<- (tp[i]+tn[i])/total
+   misclassrate[i]<- (fp[i]+fn[i])/total
+   sensitivity[i]<- tp[i]/(tp[i]+fn[i])
+   specificity[i]<- tn[i]/(fp[i]+tn[i])
+   oneminusspec[i]<- fp[i]/(fp[i]+tn[i])
+   cutoff[i]<- 0.01*(i-1)
+ }
> 
> #PLOTTING ROC CURVE
> plot(oneminusspec, sensitivity, type="l", lty=1, main="The Receiver 
+ Operating Characteristic Curve", xlab="1-Specificity", ylab="Sensitivity")
> points(oneminusspec, sensitivity, pch=0) #pch=plot character, 0=square
> 
> #REPORTING MEASURES FOR THE POINT ON ROC CURVE CLOSEST TO THE IDEAL POINT (0,1)
> distance<- c()
> for (i in 1:102)
+   distance[i]<- sqrt(oneminusspec[i]^2+(1-sensitivity[i])^2)
> 
> measures<- cbind(accuracy, misclassrate, sensitivity, specificity, distance, cutoff, tp, fp, tn, fn)
> min.dist<- min(distance)
> print(measures)
       accuracy misclassrate sensitivity specificity  distance cutoff   tp    fp    tn   fn
  [1,]  0.08500      0.91500   1.0000000   0.0000000 1.0000000   0.00 1700 18300     0    0
  [2,]  0.83665      0.16335   0.8917647   0.8315301 0.2002424   0.01 1516  3083 15217  184
  [3,]  0.83665      0.16335   0.8917647   0.8315301 0.2002424   0.02 1516  3083 15217  184
  [4,]  0.83665      0.16335   0.8917647   0.8315301 0.2002424   0.03 1516  3083 15217  184
  [5,]  0.83665      0.16335   0.8917647   0.8315301 0.2002424   0.04 1516  3083 15217  184
  [6,]  0.83665      0.16335   0.8917647   0.8315301 0.2002424   0.05 1516  3083 15217  184
  [7,]  0.83665      0.16335   0.8917647   0.8315301 0.2002424   0.06 1516  3083 15217  184
  [8,]  0.83690      0.16310   0.8917647   0.8318033 0.2000125   0.07 1516  3078 15222  184
  [9,]  0.83800      0.16200   0.8900000   0.8331694 0.1998311   0.08 1513  3053 15247  187
 [10,]  0.83920      0.16080   0.8876471   0.8346995 0.1998686   0.09 1509  3025 15275  191
 [11,]  0.84230      0.15770   0.8835294   0.8384699 0.1991415   0.10 1502  2956 15344  198
 [12,]  0.84805      0.15195   0.8741176   0.8456284 0.1991907   0.11 1486  2825 15475  214
 [13,]  0.91830      0.08170   0.7705882   0.9320219 0.2392714   0.12 1310  1244 17056  390
 [14,]  0.91830      0.08170   0.7705882   0.9320219 0.2392714   0.13 1310  1244 17056  390
 [15,]  0.91830      0.08170   0.7705882   0.9320219 0.2392714   0.14 1310  1244 17056  390
 [16,]  0.91840      0.08160   0.7705882   0.9321311 0.2392403   0.15 1310  1242 17058  390
 [17,]  0.91860      0.08140   0.7705882   0.9323497 0.2391784   0.16 1310  1238 17062  390
 [18,]  0.91910      0.08090   0.7705882   0.9328962 0.2390244   0.17 1310  1228 17072  390
 [19,]  0.91910      0.08090   0.7705882   0.9328962 0.2390244   0.18 1310  1228 17072  390
 [20,]  0.91965      0.08035   0.7670588   0.9338251 0.2421584   0.19 1304  1211 17089  396
 [21,]  0.91965      0.08035   0.7670588   0.9338251 0.2421584   0.20 1304  1211 17089  396
 [22,]  0.92185      0.07815   0.7641176   0.9365027 0.2442793   0.21 1299  1162 17138  401
 [23,]  0.92190      0.07810   0.7641176   0.9365574 0.2442651   0.22 1299  1161 17139  401
 [24,]  0.94505      0.05495   0.6647059   0.9710929 0.3365379   0.23 1130   529 17771  570
 [25,]  0.94515      0.05485   0.6647059   0.9712022 0.3365285   0.24 1130   527 17773  570
 [26,]  0.94515      0.05485   0.6647059   0.9712022 0.3365285   0.25 1130   527 17773  570
 [27,]  0.94540      0.05460   0.6641176   0.9715301 0.3370868   0.26 1129   521 17779  571
 [28,]  0.94540      0.05460   0.6641176   0.9715301 0.3370868   0.27 1129   521 17779  571
 [29,]  0.94565      0.05435   0.6629412   0.9719126 0.3382271   0.28 1127   514 17786  573
 [30,]  0.94565      0.05435   0.6629412   0.9719126 0.3382271   0.29 1127   514 17786  573
 [31,]  0.94565      0.05435   0.6629412   0.9719126 0.3382271   0.30 1127   514 17786  573
 [32,]  0.94660      0.05340   0.6611765   0.9731148 0.3398885   0.31 1124   492 17808  576
 [33,]  0.94660      0.05340   0.6611765   0.9731148 0.3398885   0.32 1124   492 17808  576
 [34,]  0.94660      0.05340   0.6611765   0.9731148 0.3398885   0.33 1124   492 17808  576
 [35,]  0.95560      0.04440   0.5847059   0.9900546 0.4154132   0.34  994   182 18118  706
 [36,]  0.95560      0.04440   0.5847059   0.9900546 0.4154132   0.35  994   182 18118  706
 [37,]  0.95560      0.04440   0.5847059   0.9900546 0.4154132   0.36  994   182 18118  706
 [38,]  0.95555      0.04445   0.5829412   0.9901639 0.4171748   0.37  991   180 18120  709
 [39,]  0.95555      0.04445   0.5829412   0.9901639 0.4171748   0.38  991   180 18120  709
 [40,]  0.95555      0.04445   0.5829412   0.9901639 0.4171748   0.39  991   180 18120  709
 [41,]  0.95555      0.04445   0.5829412   0.9901639 0.4171748   0.40  991   180 18120  709
 [42,]  0.95565      0.04435   0.5817647   0.9903825 0.4183459   0.41  989   176 18124  711
 [43,]  0.95560      0.04440   0.5811765   0.9903825 0.4189339   0.42  988   176 18124  712
 [44,]  0.95560      0.04440   0.5811765   0.9903825 0.4189339   0.43  988   176 18124  712
 [45,]  0.95560      0.04440   0.5811765   0.9903825 0.4189339   0.44  988   176 18124  712
 [46,]  0.95590      0.04410   0.5176471   0.9966120 0.4823648   0.45  880    62 18238  820
 [47,]  0.95580      0.04420   0.5164706   0.9966120 0.4835413   0.46  878    62 18238  822
 [48,]  0.95580      0.04420   0.5164706   0.9966120 0.4835413   0.47  878    62 18238  822
 [49,]  0.95580      0.04420   0.5164706   0.9966120 0.4835413   0.48  878    62 18238  822
 [50,]  0.95580      0.04420   0.5164706   0.9966120 0.4835413   0.49  878    62 18238  822
 [51,]  0.95580      0.04420   0.5164706   0.9966120 0.4835413   0.50  878    62 18238  822
 [52,]  0.95580      0.04420   0.5164706   0.9966120 0.4835413   0.51  878    62 18238  822
 [53,]  0.95580      0.04420   0.5164706   0.9966120 0.4835413   0.52  878    62 18238  822
 [54,]  0.95580      0.04420   0.5164706   0.9966120 0.4835413   0.53  878    62 18238  822
 [55,]  0.95580      0.04420   0.5164706   0.9966120 0.4835413   0.54  878    62 18238  822
 [56,]  0.95575      0.04425   0.5158824   0.9966120 0.4841295   0.55  877    62 18238  823
 [57,]  0.95455      0.04545   0.4717647   0.9993989 0.5282356   0.56  802    11 18289  898
 [58,]  0.95455      0.04545   0.4717647   0.9993989 0.5282356   0.57  802    11 18289  898
 [59,]  0.95455      0.04545   0.4717647   0.9993989 0.5282356   0.58  802    11 18289  898
 [60,]  0.95455      0.04545   0.4717647   0.9993989 0.5282356   0.59  802    11 18289  898
 [61,]  0.95455      0.04545   0.4717647   0.9993989 0.5282356   0.60  802    11 18289  898
 [62,]  0.95440      0.04560   0.4700000   0.9993989 0.5300003   0.61  799    11 18289  901
 [63,]  0.95440      0.04560   0.4700000   0.9993989 0.5300003   0.62  799    11 18289  901
 [64,]  0.95440      0.04560   0.4700000   0.9993989 0.5300003   0.63  799    11 18289  901
 [65,]  0.95440      0.04560   0.4700000   0.9993989 0.5300003   0.64  799    11 18289  901
 [66,]  0.95440      0.04560   0.4700000   0.9993989 0.5300003   0.65  799    11 18289  901
 [67,]  0.95440      0.04560   0.4700000   0.9993989 0.5300003   0.66  799    11 18289  901
 [68,]  0.95230      0.04770   0.4400000   0.9998907 0.5600000   0.67  748     2 18298  952
 [69,]  0.95230      0.04770   0.4400000   0.9998907 0.5600000   0.68  748     2 18298  952
 [70,]  0.95230      0.04770   0.4400000   0.9998907 0.5600000   0.69  748     2 18298  952
 [71,]  0.95225      0.04775   0.4394118   0.9998907 0.5605882   0.70  747     2 18298  953
 [72,]  0.95225      0.04775   0.4394118   0.9998907 0.5605882   0.71  747     2 18298  953
 [73,]  0.95225      0.04775   0.4394118   0.9998907 0.5605882   0.72  747     2 18298  953
 [74,]  0.95225      0.04775   0.4394118   0.9998907 0.5605882   0.73  747     2 18298  953
 [75,]  0.95225      0.04775   0.4394118   0.9998907 0.5605882   0.74  747     2 18298  953
 [76,]  0.95225      0.04775   0.4394118   0.9998907 0.5605882   0.75  747     2 18298  953
 [77,]  0.95225      0.04775   0.4394118   0.9998907 0.5605882   0.76  747     2 18298  953
 [78,]  0.95225      0.04775   0.4394118   0.9998907 0.5605882   0.77  747     2 18298  953
 [79,]  0.95130      0.04870   0.4276471   0.9999454 0.5723529   0.78  727     1 18299  973
 [80,]  0.95130      0.04870   0.4276471   0.9999454 0.5723529   0.79  727     1 18299  973
 [81,]  0.95130      0.04870   0.4276471   0.9999454 0.5723529   0.80  727     1 18299  973
 [82,]  0.95130      0.04870   0.4276471   0.9999454 0.5723529   0.81  727     1 18299  973
 [83,]  0.95130      0.04870   0.4276471   0.9999454 0.5723529   0.82  727     1 18299  973
 [84,]  0.95130      0.04870   0.4276471   0.9999454 0.5723529   0.83  727     1 18299  973
 [85,]  0.95130      0.04870   0.4276471   0.9999454 0.5723529   0.84  727     1 18299  973
 [86,]  0.95130      0.04870   0.4276471   0.9999454 0.5723529   0.85  727     1 18299  973
 [87,]  0.95130      0.04870   0.4276471   0.9999454 0.5723529   0.86  727     1 18299  973
 [88,]  0.95130      0.04870   0.4276471   0.9999454 0.5723529   0.87  727     1 18299  973
 [89,]  0.95130      0.04870   0.4276471   0.9999454 0.5723529   0.88  727     1 18299  973
 [90,]  0.95040      0.04960   0.4164706   1.0000000 0.5835294   0.89  708     0 18300  992
 [91,]  0.95040      0.04960   0.4164706   1.0000000 0.5835294   0.90  708     0 18300  992
 [92,]  0.95040      0.04960   0.4164706   1.0000000 0.5835294   0.91  708     0 18300  992
 [93,]  0.95040      0.04960   0.4164706   1.0000000 0.5835294   0.92  708     0 18300  992
 [94,]  0.95040      0.04960   0.4164706   1.0000000 0.5835294   0.93  708     0 18300  992
 [95,]  0.95040      0.04960   0.4164706   1.0000000 0.5835294   0.94  708     0 18300  992
 [96,]  0.95040      0.04960   0.4164706   1.0000000 0.5835294   0.95  708     0 18300  992
 [97,]  0.95040      0.04960   0.4164706   1.0000000 0.5835294   0.96  708     0 18300  992
 [98,]  0.95040      0.04960   0.4164706   1.0000000 0.5835294   0.97  708     0 18300  992
 [99,]  0.95040      0.04960   0.4164706   1.0000000 0.5835294   0.98  708     0 18300  992
[100,]  0.95040      0.04960   0.4164706   1.0000000 0.5835294   0.99  708     0 18300  992
 [ reached 'max' / getOption("max.print") -- omitted 2 rows ]
> 
> #COMPUTING AREA UNDER THE ROC CURVE
> sensitivity<- sensitivity[order(sensitivity)]
> oneminusspec<- oneminusspec[order(oneminusspec)]
> 
> library(Hmisc) #Harrell Miscellaneous packages
> lagx<- Lag(oneminusspec,shift=1)
> lagy<- Lag(sensitivity, shift=1)
> lagx[is.na(lagx)]<- 0
> lagy[is.na(lagy)]<- 0
> trapezoid<- (oneminusspec-lagx)*(sensitivity+lagy)/2
> print(AUC<- sum(trapezoid))
[1] 0.9153317